{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11287173,"sourceType":"datasetVersion","datasetId":7057290},{"sourceId":11322065,"sourceType":"datasetVersion","datasetId":7081499}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q x-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:07.968237Z","iopub.execute_input":"2025-04-08T11:40:07.968581Z","iopub.status.idle":"2025-04-08T11:40:12.833796Z","shell.execute_reply.started":"2025-04-08T11:40:07.968552Z","shell.execute_reply":"2025-04-08T11:40:12.832776Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nimport sympy\n\nfrom sympy import expand\nfrom sympy import sympify\nfrom sympy import series\nfrom sympy import Symbol, symbols\nfrom sympy import im, I\n\nfrom tqdm import tqdm\n\nimport re\n\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:12.834958Z","iopub.execute_input":"2025-04-08T11:40:12.835185Z","iopub.status.idle":"2025-04-08T11:40:16.511817Z","shell.execute_reply.started":"2025-04-08T11:40:12.835164Z","shell.execute_reply":"2025-04-08T11:40:16.511080Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"BOS_IDX = 0\nPAD_IDX = 1\nUNK_IDX = 2\nEOS_IDX = 11","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:16.513412Z","iopub.execute_input":"2025-04-08T11:40:16.513914Z","iopub.status.idle":"2025-04-08T11:40:16.517553Z","shell.execute_reply.started":"2025-04-08T11:40:16.513882Z","shell.execute_reply":"2025-04-08T11:40:16.516696Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from dataclasses import dataclass, field, fields\nfrom typing import Optional\n\n@dataclass\nclass Config:\n    experiment_name: Optional[str] = \"seq2seq_transformer\"\n    root_dir: Optional[str] = \"./\"\n    device: Optional[str] = \"cuda:0\"\n        \n    #training parameters\n    epochs: Optional[int] = 100\n    seed: Optional[int] = 42\n    use_half_precision: Optional[bool] = True\n\n    # scheduler parameters\n    scheduler_type: Optional[str] = \"cosine_annealing_warm_restart\" # multi_step or none\n    T_0: Optional[int] = 10\n    T_mult: Optional[int] = 1\n\n    # optimizer parameters\n    optimizer_type: Optional[str] = \"adam\" # sgd or adam\n    optimizer_lr: Optional[float] = 0.0001   \n    optimizer_momentum: Optional[float] = 0.9\n    optimizer_weight_decay: Optional[float] = 0.0001\n    optimizer_no_decay: Optional[list] = field(default_factory=list)\n    clip_grad_norm: Optional[float] = -1\n        \n    # Model Parameters\n    model_name: Optional[str] = \"seq2seq_transformer\"\n    hybrid: Optional[bool] = True\n    embedding_size: Optional[int] = 128\n    hidden_dim: Optional[int] = 128\n    pff_dim: Optional[int] = 512\n    nhead: Optional[int] = 8\n    num_encoder_layers: Optional[int] = 2\n    num_decoder_layers: Optional[int] = 6\n    dropout: Optional[int] = 0.2\n    pretrain: Optional[bool] = False\n    input_emb_size: Optional[int] = 64\n    max_input_points: Optional[int] = 210\n    src_vocab_size: Optional[int] = 32\n    tgt_vocab_size: Optional[int] = 22\n\n    # Criterion\n    criterion: Optional[str] = \"cross_entropy\"\n        \n    def print_config(self):\n        print(\"=\"*50+\"\\nConfig\\n\"+\"=\"*50)\n        for field in fields(self):\n            print(field.name.ljust(30), getattr(self, field.name))\n        print(\"=\"*50)\n\n    def save(self, root_dir):\n        path = root_dir + \"/config.txt\"\n        with open(path, \"w\") as f:\n            f.write(\"=\"*50+\"\\nConfig\\n\"+\"=\"*50 + \"\\n\")\n            for field in fields(self):\n                f.write(field.name.ljust(30) + \": \" + str(getattr(self, field.name)) + \"\\n\")\n            f.write(\"=\"*50)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:16.518841Z","iopub.execute_input":"2025-04-08T11:40:16.519078Z","iopub.status.idle":"2025-04-08T11:40:16.535178Z","shell.execute_reply.started":"2025-04-08T11:40:16.519049Z","shell.execute_reply":"2025-04-08T11:40:16.534448Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Load and clean","metadata":{}},{"cell_type":"code","source":"pth = '/kaggle/input/data-no-dup/final_data_6519.csv'\n\ndf = pd.read_csv(pth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:16.535774Z","iopub.execute_input":"2025-04-08T11:40:16.535996Z","iopub.status.idle":"2025-04-08T11:40:16.611119Z","shell.execute_reply.started":"2025-04-08T11:40:16.535977Z","shell.execute_reply":"2025-04-08T11:40:16.610459Z"},"_kg_hide-input":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def spt(i, order=4):\n    # print(df['expansion'].iloc[i])\n    # return df['expansion'].iloc[i].split('x**')\n    expr = sympify(df['expansion'].iloc[i]).evalf(4).as_poly()\n    # print(expr)\n    # print(expr.free_symbols)\n    coeffs = expr.all_coeffs()[::-1]\n    if len(coeffs) < order + 1:\n        coeffs += [0]*(order - len(coeffs) + 1)\n    return coeffs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:16.611968Z","iopub.execute_input":"2025-04-08T11:40:16.612283Z","iopub.status.idle":"2025-04-08T11:40:16.616719Z","shell.execute_reply.started":"2025-04-08T11:40:16.612251Z","shell.execute_reply":"2025-04-08T11:40:16.615936Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df_clean = pd.DataFrame(columns = df.columns)\ncoeffs = []\nfor idx, row in tqdm(df.iterrows(),total=len(df)):\n    try:\n        coeff = spt(idx)\n        coeffs.append(coeff)\n        df_clean.loc[len(df_clean)] = row\n    except Exception as ex:\n        continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:16.617502Z","iopub.execute_input":"2025-04-08T11:40:16.617820Z","iopub.status.idle":"2025-04-08T11:40:39.117048Z","shell.execute_reply.started":"2025-04-08T11:40:16.617793Z","shell.execute_reply":"2025-04-08T11:40:39.115817Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 6519/6519 [00:22<00:00, 289.98it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df_clean['coefficients'] = coeffs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.120832Z","iopub.execute_input":"2025-04-08T11:40:39.121066Z","iopub.status.idle":"2025-04-08T11:40:39.127085Z","shell.execute_reply.started":"2025-04-08T11:40:39.121045Z","shell.execute_reply":"2025-04-08T11:40:39.126150Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\n\ndef split_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n    val_size = val_ratio / (val_ratio + test_ratio)\n    val_data, test_data = train_test_split(temp_data, train_size=val_size, random_state=42)\n\n    data = {\n        'train': train_data,\n        'valid': val_data,\n        'test': test_data\n    }\n    return train_data, val_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.129231Z","iopub.execute_input":"2025-04-08T11:40:39.129482Z","iopub.status.idle":"2025-04-08T11:40:39.763257Z","shell.execute_reply.started":"2025-04-08T11:40:39.129449Z","shell.execute_reply":"2025-04-08T11:40:39.762605Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def replace_exponent(expr: str) -> str:\n    # Step 1: Replace powers **2 to **4 with ^2 to ^4\n    expr = re.sub(r'(\\b\\w+\\b)\\s*\\*\\*\\s*([2-4])', r'<\\1^\\2>', expr)\n \n    # Step 2: Replace plain variables with variable_1 (not touching already transformed ones)\n    # Negative lookbehind for ^ or _, to avoid changing x^2 or x_1\n    expr = re.sub(r'(?<![\\^_])\\bx\\b(?![\\^_])', '<x^1>', expr)\n\n    return expr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.764071Z","iopub.execute_input":"2025-04-08T11:40:39.764515Z","iopub.status.idle":"2025-04-08T11:40:39.768820Z","shell.execute_reply.started":"2025-04-08T11:40:39.764491Z","shell.execute_reply":"2025-04-08T11:40:39.767887Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def dec_preproc(input_str, num_token='<NUM>'):\n    \n    exp = input_str\n    e = sympy.Symbol('e')\n    expr = str(sympify(exp).evalf(6,subs={'e':sympy.core.numbers.E})).replace(' ','')\n    expr = replace_exponent(expr)\n    expr_arr = expr.replace('+', ' + ').replace('*',' * ').replace('-',' - ').split(' ')\n\n    expr_mod = []\n\n    def check_float(f):\n        try:\n            _ = float(f)\n            return True\n        except (ValueError, TypeError):\n            return False\n    \n    for i in expr_arr:\n        if i == '':\n            continue\n\n        if check_float(i) or check_float(i[:-1]):\n            for char in str(i):\n                expr_mod.append(char)\n        else:\n            expr_mod.append(i)\n    return expr_mod","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.769719Z","iopub.execute_input":"2025-04-08T11:40:39.770010Z","iopub.status.idle":"2025-04-08T11:40:39.780792Z","shell.execute_reply.started":"2025-04-08T11:40:39.769983Z","shell.execute_reply":"2025-04-08T11:40:39.780091Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Tokenizers","metadata":{}},{"cell_type":"code","source":"src_vocab = []\n\nsrc_vocab += ['<PAD>', '<SOS>', '<UNK>', 'x']\nsrc_vocab += [str(i) for i in range(10)]\nsrc_vocab += ['pi', 's+', 's-', 'E']\nsrc_vocab += ['add', 'mul', 'pow']\nsrc_vocab += ['sin', 'cos', 'tan', 'cot']\nsrc_vocab += ['asin', 'acos', 'atan', 'acot']\nsrc_vocab += ['ln', 'exp']\nsrc_vocab += ['<EOS>']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.781630Z","iopub.execute_input":"2025-04-08T11:40:39.781938Z","iopub.status.idle":"2025-04-08T11:40:39.793595Z","shell.execute_reply.started":"2025-04-08T11:40:39.781907Z","shell.execute_reply":"2025-04-08T11:40:39.792921Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"tgt_vocab = []\n\ntgt_vocab += ['<PAD>', '<SOS>', '<UNK>']\ntgt_vocab += [f'<x^{i}>' for i in range(1,5)]\ntgt_vocab += [str(i) for i in range(10)]\ntgt_vocab += ['*', '+', '-', '.']\ntgt_vocab += ['<EOS>']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.794148Z","iopub.execute_input":"2025-04-08T11:40:39.794342Z","iopub.status.idle":"2025-04-08T11:40:39.805390Z","shell.execute_reply.started":"2025-04-08T11:40:39.794322Z","shell.execute_reply":"2025-04-08T11:40:39.804738Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"len(tgt_vocab), len(src_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.806119Z","iopub.execute_input":"2025-04-08T11:40:39.806346Z","iopub.status.idle":"2025-04-08T11:40:39.821975Z","shell.execute_reply.started":"2025-04-08T11:40:39.806315Z","shell.execute_reply":"2025-04-08T11:40:39.821170Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(22, 32)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self, vocab):\n        # Initialize with default vocabulary if none provided\n        self.vocab = vocab\n        \n        # Create mappings\n        self.token_to_idx = {token: idx for idx, token in enumerate(self.vocab)}\n        self.idx_to_token = {idx: token for idx, token in enumerate(self.vocab)}\n        \n        # Special tokens\n        self.pad_idx = self.token_to_idx['<PAD>']\n        self.sos_idx = self.token_to_idx['<SOS>']\n        self.eos_idx = self.token_to_idx['<EOS>']\n        self.unk_idx = self.token_to_idx['<UNK>']\n    \n    def encode(self, tokens, add_special_tokens=True, max_length=None):\n        \"\"\"\n        Encode a list of tokens into indices\n        \n        Args:\n            tokens (list): List of tokens to encode\n            add_special_tokens (bool): Whether to add SOS and EOS tokens\n            max_length (int, optional): Maximum length to pad/truncate to\n            \n        Returns:\n            list: List of token indices\n        \"\"\"\n        if add_special_tokens:\n            tokens = ['<SOS>'] + tokens + ['<EOS>']\n        \n        # Convert tokens to indices\n        indices = [self.token_to_idx.get(token, self.unk_idx) for token in tokens]\n        \n        # Handle padding/truncation if max_length specified\n        if max_length is not None:\n            if len(indices) < max_length:\n                # Pad sequence\n                indices += [self.pad_idx] * (max_length - len(indices))\n            else:\n                # Truncate sequence\n                indices = indices[:max_length]\n        \n        return indices\n    \n    def decode(self, indices, remove_special_tokens=True):\n        \"\"\"\n        Decode a list of indices back into tokens\n        \n        Args:\n            indices (list): List of indices to decode\n            remove_special_tokens (bool): Whether to remove special tokens\n            \n        Returns:\n            list: List of decoded tokens\n        \"\"\"\n        # Convert indices to tokens\n        tokens = [self.idx_to_token.get(idx, '<UNK>') for idx in indices]\n        \n        # Remove special tokens if requested\n        if remove_special_tokens:\n            tokens = [token for token in tokens if token not in ['<PAD>', '<SOS>', '<EOS>']]\n        \n        return tokens\n    \n    def batch_encode(self, batch_tokens, add_special_tokens=True, max_length=None, return_tensors=False):\n        \"\"\"\n        Encode a batch of token lists\n        \n        Args:\n            batch_tokens (list): List of token lists to encode\n            add_special_tokens (bool): Whether to add SOS and EOS tokens\n            max_length (int, optional): Maximum length to pad/truncate to\n            return_tensors (bool): Whether to return PyTorch tensors\n            \n        Returns:\n            list or torch.Tensor: Batch of encoded sequences\n        \"\"\"\n        encoded_batch = [self.encode(tokens, add_special_tokens, max_length) for tokens in batch_tokens]\n        \n        # If max_length not specified, pad to the longest sequence in batch\n        if max_length is None and encoded_batch:\n            max_len = max(len(seq) for seq in encoded_batch)\n            encoded_batch = [seq + [self.pad_idx] * (max_len - len(seq)) for seq in encoded_batch]\n        \n        # Convert to tensors if requested\n        if return_tensors:\n            import torch\n            encoded_batch = torch.tensor(encoded_batch, dtype=torch.long)\n        \n        return encoded_batch\n    \n    def save_vocabulary(self, filepath):\n        \"\"\"Save vocabulary to a file\"\"\"\n        with open(filepath, 'w') as f:\n            for token in self.vocab:\n                f.write(f\"{token}\\n\")\n        print(f\"Vocabulary saved to {filepath}\")\n    \n    @classmethod\n    def from_file(cls, filepath):\n        \"\"\"Load vocabulary from a file\"\"\"\n        with open(filepath, 'r') as f:\n            vocab = [line.strip() for line in f]\n        return cls(vocab)\n    \n    def __len__(self):\n        \"\"\"Return the size of the vocabulary\"\"\"\n        return len(self.vocab)\n    \n    def add_tokens(self, new_tokens):\n        \"\"\"\n        Add new tokens to the vocabulary\n        \n        Args:\n            new_tokens (list): List of tokens to add\n            \n        Returns:\n            int: Number of tokens added\n        \"\"\"\n        tokens_added = 0\n        for token in new_tokens:\n            if token not in self.vocab:\n                self.vocab.append(token)\n                self.token_to_idx[token] = len(self.vocab) - 1\n                self.idx_to_token[len(self.vocab) - 1] = token\n                tokens_added += 1\n        \n        return tokens_added","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.822986Z","iopub.execute_input":"2025-04-08T11:40:39.823261Z","iopub.status.idle":"2025-04-08T11:40:39.835026Z","shell.execute_reply.started":"2025-04-08T11:40:39.823234Z","shell.execute_reply":"2025-04-08T11:40:39.834326Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# data handling","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Taylor_data(Dataset):\n    \n    def __init__(self, df, src_vocab, tgt_vocab):\n        self.enc_tokenizer = Tokenizer(src_vocab)\n        self.dec_tokenizer = Tokenizer(tgt_vocab)\n\n        self.df = df\n        \n        self.src = []\n        self.tgt = []\n        self.build_dataset()\n    \n    def build_dataset(self):\n        for idx, row in tqdm(self.df.iterrows(), total=len(self.df)):\n            if len(row['prefix']) > 200:\n                continue\n                \n            tgt_preproc = dec_preproc(row['expansion'])\n            src_ids = self.enc_tokenizer.encode(eval(row['prefix']))\n            tgt_ids = self.dec_tokenizer.encode(tgt_preproc)\n            \n            self.src.append(src_ids)\n            self.tgt.append(tgt_ids)\n        \n        print('Built Dataset')\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, idx):\n\n        return torch.tensor(self.src[idx]).long(), torch.tensor(self.tgt[idx]).long()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.835857Z","iopub.execute_input":"2025-04-08T11:40:39.836098Z","iopub.status.idle":"2025-04-08T11:40:39.852006Z","shell.execute_reply.started":"2025-04-08T11:40:39.836080Z","shell.execute_reply":"2025-04-08T11:40:39.851397Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def collate_fn(batch):\n    src_batch, tgt_batch, num_batch = [], [], []\n    for (src_sample, tgt_sample) in batch:\n        \n        src_batch.append(src_sample)\n        tgt_batch.append(tgt_sample)\n    \n\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n    return src_batch, tgt_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.852824Z","iopub.execute_input":"2025-04-08T11:40:39.853098Z","iopub.status.idle":"2025-04-08T11:40:39.863144Z","shell.execute_reply.started":"2025-04-08T11:40:39.853069Z","shell.execute_reply":"2025-04-08T11:40:39.862367Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_dataloaders(datasets, train_bs, val_bs, test_bs):\n    \"\"\"\n    Get data loaders for training, validation, and testing.\n\n    Args:\n    - datasets: Dictionary containing train, validation, and test datasets\n    - train_bs: Batch size for training\n    - val_bs: Batch size for validation\n    - test_bs: Batch size for testing\n\n    Returns:\n    - dataloaders: Dictionary containing train, validation, and test data loaders\n    \"\"\"\n    train_dataloader = DataLoader(datasets['train'], batch_size=train_bs,\n                                  shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n    val_dataloader = DataLoader(datasets['valid'], batch_size=val_bs,\n                                  shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n    test_dataloader = DataLoader(datasets['test'], batch_size=test_bs,\n                                  shuffle=False, num_workers=2, pin_memory=False, collate_fn=collate_fn)\n    \n    dataloaders = {\n        \"train\":train_dataloader,\n        \"test\":test_dataloader,\n        \"valid\":val_dataloader\n        }\n    \n    return dataloaders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.863886Z","iopub.execute_input":"2025-04-08T11:40:39.864143Z","iopub.status.idle":"2025-04-08T11:40:39.878467Z","shell.execute_reply.started":"2025-04-08T11:40:39.864124Z","shell.execute_reply":"2025-04-08T11:40:39.877729Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Trainer utils","metadata":{}},{"cell_type":"code","source":"import os\nimport random\n\nimport torch\nimport numpy as np\n\n\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\ndef generate_square_subsequent_mask(sz, device):\n    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\ndef create_mask(src, tgt, device):\n    src_seq_len = src.shape[1]\n    tgt_seq_len = tgt.shape[1]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n\n    src_padding_mask = (torch.zeros((src.shape[0], src_seq_len), device=device)).type(torch.bool)\n    tgt_padding_mask = (tgt == PAD_IDX)\n    tgt_mask = tgt_mask\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n\ndef sequence_accuracy(y_pred, y_true):\n\n    count = 0\n    total = len(y_pred)\n    for (predicted_tokens, original_tokens) in zip(y_pred, y_true):\n        original_tokens = original_tokens.tolist()\n        predicted_tokens = predicted_tokens.tolist()\n        if original_tokens == predicted_tokens:\n            count = count+1\n\n    return count/total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.879298Z","iopub.execute_input":"2025-04-08T11:40:39.879585Z","iopub.status.idle":"2025-04-08T11:40:39.894907Z","shell.execute_reply.started":"2025-04-08T11:40:39.879566Z","shell.execute_reply":"2025-04-08T11:40:39.894096Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torch.nn import Transformer\nimport math\n\n# https://github.com/neerajanand321/SYMBA_Pytorch/blob/main/models/seq2seq_transformer.py\nclass TokenEmbedding(nn.Module):\n    ''' helper Module to convert tensor of input indices into corresponding tensor of token embeddings'''\n    \n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\nclass PositionalEncoding(nn.Module):\n    ''' helper Module that adds positional encoding to the token embedding to introduce a notion of word order.'''\n    \n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(0)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n\n\nclass Model(nn.Module):\n    '''Seq2Seq Network'''\n    \n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 input_emb_size: int,\n                 max_input_points: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1,):\n        super(Model, self).__init__()\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout,\n                                       batch_first=True)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.src_tok_emb(src)\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n\n        \n        \n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.src_tok_emb(src), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.895853Z","iopub.execute_input":"2025-04-08T11:40:39.896089Z","iopub.status.idle":"2025-04-08T11:40:39.913598Z","shell.execute_reply.started":"2025-04-08T11:40:39.896059Z","shell.execute_reply":"2025-04-08T11:40:39.912911Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Predictor","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\n\nclass Predictor:\n    \"\"\"\n    Predictor class for generating predictions using a trained model.\n    \"\"\"\n    def __init__(self, config):\n        \"\"\"\n        Initialize Predictor object.\n\n        Args:\n        - config: Configuration object containing model parameters\n        \"\"\"\n        self.config = config\n        self.device = torch.device(self.config.device)\n\n        # Get the model\n        self.model = self.get_model()\n        self.model.to(self.device)\n\n        # Load the best checkpoint\n        self.logs_dir = os.path.join(self.config.root_dir, self.config.experiment_name)\n        path = os.path.join(self.logs_dir, \"best_checkpoint.pth\")\n        self.model.load_state_dict(torch.load(path)[\"state_dict\"])\n        \n        # Set the model to evaluation mode\n        self.model.eval()\n        \n    def get_model(self):\n        if self.config.model_name == \"seq2seq_transformer\":\n            model = Model(num_encoder_layers=self.config.num_encoder_layers,\n                          num_decoder_layers=self.config.num_decoder_layers,\n                          emb_size=self.config.embedding_size,\n                          nhead=self.config.nhead,\n                          src_vocab_size=self.config.src_vocab_size,\n                          tgt_vocab_size=self.config.tgt_vocab_size,\n                          input_emb_size=self.config.input_emb_size,\n                          max_input_points=self.config.max_input_points,\n                          )\n\n        \n        return model\n    \n    def generate_square_subsequent_mask(self, sz, device):\n        mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n    \n    def greedy_decode(self, src, src_mask, max_len, start_symbol, src_padding_mask=None):\n        src = src.to(self.device)\n        src_mask = src_mask.to(self.device)\n        src_padding_mask = src_padding_mask.to(self.device)\n        dim = 1\n\n        memory = self.model.encode(src, src_mask)\n        memory = memory.to(self.device)\n        dim = 1\n        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(self.device)\n        for i in range(max_len-1):\n\n            tgt_mask = (self.generate_square_subsequent_mask(ys.size(1), self.device).type(torch.bool)).to(self.device)\n\n            out = self.model.decode(ys, memory, tgt_mask)\n            prob = self.model.generator(out[:, -1])\n\n            _, next_word = torch.max(prob, dim=1)\n            next_word = next_word.item()\n\n            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=dim)\n            if next_word == EOS_IDX:\n                break\n\n        return ys\n\n\n    def predict(self, x):\n        self.model.eval()\n        \n        if self.config.model_name == \"seq2seq_transformer\":\n            src = x\n            num_tokens = src.shape[1]\n\n            src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n            src_padding_mask = torch.zeros(1, num_tokens).type(torch.bool)\n            tgt_tokens = self.greedy_decode(src, src_mask, max_len=256, start_symbol=BOS_IDX, src_padding_mask=src_padding_mask).flatten()\n\n            return tgt_tokens\n        else:\n            ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(self.device)\n            e_mask = torch.zeros(1, x.shape[1]).type(torch.bool).to(self.device)\n            memory = self.model.encoder(x, e_mask)\n\n            for idx in range(1, 256):\n                d_mask = torch.triu(torch.full((ys.size(1), ys.size(1)), float('-inf')), diagonal=1).to(self.device)\n                d_out = self.model.decoder(ys, memory, e_mask, d_mask)\n\n                prob = self.model.generator(d_out[:, -1])\n                _, next_word = torch.max(prob, dim=1)\n                next_word = next_word.item()\n                ys = torch.cat([ys, torch.ones(1, 1).type_as(x.data).fill_(next_word)], dim=1)\n                if next_word == EOS_IDX:\n                    break\n\n            return ys.flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.914416Z","iopub.execute_input":"2025-04-08T11:40:39.914665Z","iopub.status.idle":"2025-04-08T11:40:39.930148Z","shell.execute_reply.started":"2025-04-08T11:40:39.914635Z","shell.execute_reply":"2025-04-08T11:40:39.929460Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n\nclass Trainer:\n    \"\"\"\n    Trainer class for training and evaluating a PyTorch model.\n    \"\"\"\n    def __init__(self, config, dataloaders):\n        \"\"\"\n        Initialize Trainer object.\n\n        Args:\n        - config: Configuration object containing training parameters\n        - dataloaders: Dictionary containing data loaders for train, validation, and test sets\n        \"\"\"\n        self.config = config\n        self.device = torch.device(self.config.device)\n        self.dataloaders = dataloaders\n\n        seed_everything(self.config.seed)\n\n        self.scaler = torch.cuda.amp.GradScaler()\n        if self.config.use_half_precision:\n            self.dtype = torch.float16\n        else:\n            self.dtype = torch.float32\n\n        # Initialize model, optimizer, scheduler, and criterion\n        self.model = self.get_model()\n        self.model.to(self.device)\n        self.optimizer = self.get_optimizer()\n        self.scheduler = self.get_scheduler()\n        self.criterion = self.get_criterion()\n\n        # Initialize training-related variables\n        self.current_epoch = 0\n        self.best_accuracy = -1\n        self.best_val_loss = 1e6\n        self.train_loss_list = []\n        self.valid_loss_list = []\n        self.valid_accuracy_tok_list = []\n\n        # Create directory for saving logs\n        self.logs_dir = os.path.join(self.config.root_dir, self.config.experiment_name)\n        os.makedirs(self.logs_dir, exist_ok=True)\n\n    def get_model(self):\n        \"\"\"\n        Initialize and return the model based on the configuration.\n        \"\"\"\n        if self.config.model_name == \"seq2seq_transformer\":\n            model = Model(num_encoder_layers=self.config.num_encoder_layers,\n                          num_decoder_layers=self.config.num_decoder_layers,\n                          emb_size=self.config.embedding_size,\n                          nhead=self.config.nhead,\n                          src_vocab_size=self.config.src_vocab_size,\n                          tgt_vocab_size=self.config.tgt_vocab_size,\n                          input_emb_size=self.config.input_emb_size,\n                          max_input_points=self.config.max_input_points,\n                          )\n\n        return model\n\n    def get_optimizer(self):\n        \"\"\"\n        Initialize and return the optimizer based on the configuration.\n        \"\"\"\n        optimizer_parameters = self.model.parameters()\n\n        if self.config.optimizer_type == \"sgd\":\n            optimizer = torch.optim.SGD(optimizer_parameters, lr=self.config.optimizer_lr, momentum=self.config.optimizer_momentum,)\n        elif self.config.optimizer_type == \"adam\":\n            optimizer = torch.optim.Adam(optimizer_parameters, lr=self.config.optimizer_lr, eps=1e-8, weight_decay=self.config.optimizer_weight_decay)\n        elif self.config.optimizer_type == \"adamw\":\n            optimizer = torch.optim.AdamW(optimizer_parameters, lr=self.config.optimizer_lr, eps=1e-8, weight_decay=self.config.optimizer_weight_decay)\n        else:\n            raise NotImplementedError\n        \n        return optimizer\n    \n    def get_scheduler(self):\n        \"\"\"\n        Initialize and return the learning rate scheduler based on the configuration.\n        \"\"\"\n        if self.config.scheduler_type == \"multi_step\":\n            scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=self.config.scheduler_milestones, gamma=self.config.scheduler_gamma)\n        elif self.config.scheduler_type == \"reduce_lr_on_plateau\":\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=2)\n        elif self.config.scheduler_type == \"cosine_annealing_warm_restart\":\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, self.config.T_0, self.config.T_mult)\n        elif self.config.scheduler_type == \"none\":\n            scheduler = None\n        else:\n            raise NotImplementedError\n        \n        return scheduler\n\n    \n    def get_criterion(self):\n        \"\"\"\n        Initialize and return the loss function based on the configuration.\n        \"\"\"\n        if self.config.criterion == \"cross_entropy\":\n            criterion = torch.nn.CrossEntropyLoss()\n        else:\n            raise NotImplementedError\n        \n        return criterion\n\n    def train_one_epoch(self):\n        \"\"\"\n        Train the model for one epoch.\n        \"\"\"\n        self.model.train()\n        pbar = tqdm(self.dataloaders['train'], total=len(self.dataloaders['train']))\n        pbar.set_description(f\"[{self.current_epoch+1}/{self.config.epochs}] Train\")\n        running_loss = AverageMeter()\n        for src, tgt in pbar:\n            src = src.to(self.device)\n            tgt = tgt.to(self.device)\n\n            bs = src.size(0)\n\n            with torch.autocast(device_type='cuda', dtype=self.dtype):\n                if self.config.model_name == \"seq2seq_transformer\":\n                    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:, :-1], self.device)\n                    logits = self.model(src, tgt[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n                    loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n                else:\n                    logits = self.model(src, tgt[:, :-1])\n                    loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n                \n            running_loss.update(loss.item(), bs)\n            pbar.set_postfix(loss=running_loss.avg)\n            \n            self.optimizer.zero_grad()\n            self.scaler.scale(loss).backward()\n\n            if self.config.clip_grad_norm > 0:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_grad_norm)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n\n        return running_loss.avg\n\n    def evaluate(self, phase):\n        \"\"\"\n        Evaluate the model on validation or test data.\n\n        Args:\n        - phase: Phase of evaluation, either \"valid\" or \"test\".\n\n        Returns:\n        - Tuple containing average token accuracy and average loss.\n        \"\"\"\n        self.model.eval()\n        \n        pbar = tqdm(self.dataloaders[phase], total=len(self.dataloaders[phase]))\n        pbar.set_description(f\"[{self.current_epoch+1}/{self.config.epochs}] {phase.capitalize()}\")\n        running_loss = AverageMeter()\n        running_acc_tok = AverageMeter()\n        \n        \n        for src, tgt in pbar:\n            src = src.to(self.device)\n            tgt = tgt.to(self.device)\n            bs = src.size(0)\n            \n            with torch.autocast(device_type='cuda', dtype=self.dtype):\n                if self.config.model_name == \"seq2seq_transformer\":\n                    with torch.no_grad():\n                        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:, :-1], self.device)\n                        logits = self.model(src, tgt[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n                        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n                else:\n                    with torch.no_grad():\n                        logits = self.model(src, tgt[:, :-1])\n                        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n\n            y_pred = torch.argmax(logits.reshape(-1, logits.shape[-1]), 1)\n            correct = (y_pred == tgt[:, 1:].reshape(-1)).cpu().numpy().mean()\n            \n            running_loss.update(loss.item(), bs)\n            running_acc_tok.update(correct, bs)\n            \n        return running_acc_tok.avg, running_loss.avg\n\n    def train(self):\n        \"\"\"\n        Main training loop.\n        \"\"\"\n        start_epoch = self.current_epoch\n        for self.current_epoch in range(start_epoch, self.config.epochs):\n            training_loss = self.train_one_epoch() \n            valid_accuracy_tok, valid_loss = self.evaluate(\"valid\")\n            \n            self.train_loss_list.append(round(training_loss, 7))\n            self.valid_loss_list.append(round(valid_loss, 7))\n            self.valid_accuracy_tok_list.append(round(valid_accuracy_tok, 7))\n            \n            if self.scheduler == \"multi_step\":\n                self.scheduler.step()\n            elif self.scheduler == \"reduce_lr_on_plateau\":\n                self.scheduler.step(valid_loss)\n                \n            if valid_loss<self.best_val_loss:\n                self.best_val_loss = valid_loss\n\n            self.save_model(\"last_checkpoint.pth\")\n\n            if valid_accuracy_tok > self.best_accuracy:\n                print(f\"==> Best Accuracy improved to {round(valid_accuracy_tok, 7)} from {self.best_accuracy}\")\n                self.best_accuracy = round(valid_accuracy_tok, 7)\n                self.save_model(\"best_checkpoint.pth\")\n            \n            self.log_results()\n\n        \n    def save_model(self, file_name):\n        \"\"\"\n        Save model checkpoints.\n        \"\"\"\n        state_dict = self.model.state_dict()\n        torch.save({\n                \"epoch\": self.current_epoch + 1,\n                \"state_dict\": state_dict,\n                'optimizer': self.optimizer.state_dict(),\n                \"train_loss_list\": self.train_loss_list,\n                \"valid_loss_list\": self.valid_loss_list,\n                \"valid_accuracy_tok_list\": self.valid_accuracy_tok_list,\n            }, os.path.join(self.logs_dir, file_name))\n\n    def log_results(self):\n        \"\"\"\n        Log training results to a CSV file.\n        \"\"\"\n        data_list = [self.train_loss_list, self.valid_loss_list, self.valid_accuracy_tok_list]\n        column_list = ['train_losses', 'valid_losses', 'token_valid_accuracy']\n        \n        df_data = np.array(data_list).T\n        df = pd.DataFrame(df_data, columns=column_list)\n        df.to_csv(os.path.join(self.logs_dir, \"logs.csv\"))\n        \n    def test_seq_acc(self):\n        \"\"\"\n        Evaluate model's sequence accuracy on test data.\n        \"\"\"\n        file = os.path.join(self.logs_dir, \"best_checkpoint.pth\")\n        state_dict = torch.load(file, map_location=self.device)['state_dict']\n        self.model.load_state_dict(state_dict)\n        \n        test_accuracy_tok, _ = self.evaluate(\"test\")\n        \n        predictor = Predictor(self.config)\n        \n        print(\"Calculating Sequence Accuracy for predictions\")\n        pbar = tqdm(self.dataloaders[\"test\"], total=len(self.dataloaders[\"test\"]))\n        pbar.set_description(f\"Test\")\n        \n        y_preds = []\n        y_true = []\n        for src, tgt in pbar:\n            src = src.to(self.device)\n            tgt = tgt.numpy()\n            bs = src.size(0)\n            y_pred = predictor.predict(src[0].unsqueeze(0)) #only one example from each batch\n            y_preds.append(y_pred.cpu().numpy())\n            y_true.append(np.trim_zeros(tgt[0]))\n        print(y_preds[1], y_true[1])\n        test_accuracy_seq = sequence_accuracy(y_true, y_preds)\n        f= open(os.path.join(self.logs_dir, \"score.txt\"),\"w+\")\n        f.write(f\"Token Accuracy = {(round(test_accuracy_tok, 7))}\\n\")\n        f.write(f\"Sequence Accuracy = {(round(test_accuracy_seq, 7))}\\n\")\n        f.close()\n        print(f\"Test Accuracy: {round(test_accuracy_tok, 7)} | Valid Accuracy: {self.best_accuracy}\") \n        print(f\"Test Sequence Accuracy: {test_accuracy_seq}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.930868Z","iopub.execute_input":"2025-04-08T11:40:39.931096Z","iopub.status.idle":"2025-04-08T11:40:39.957233Z","shell.execute_reply.started":"2025-04-08T11:40:39.931065Z","shell.execute_reply":"2025-04-08T11:40:39.956413Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def main():\ndf_train, df_valid, df_test = split_data(df_clean)\ndatasets = {\n    'train': Taylor_data(df_train, src_vocab, tgt_vocab),\n    'valid': Taylor_data(df_valid, src_vocab, tgt_vocab),\n    'test': Taylor_data(df_test, src_vocab, tgt_vocab)\n}\n# dataloaders = get_dataloaders(datasets, )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:39.960051Z","iopub.execute_input":"2025-04-08T11:40:39.960243Z","iopub.status.idle":"2025-04-08T11:40:55.974309Z","shell.execute_reply.started":"2025-04-08T11:40:39.960227Z","shell.execute_reply":"2025-04-08T11:40:55.973431Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4064/4064 [00:11<00:00, 357.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Built Dataset\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 871/871 [00:02<00:00, 385.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Built Dataset\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 872/872 [00:02<00:00, 368.93it/s]","output_type":"stream"},{"name":"stdout","text":"Built Dataset\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"dataloaders = get_dataloaders(datasets, 16, 128, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:55.975416Z","iopub.execute_input":"2025-04-08T11:40:55.975752Z","iopub.status.idle":"2025-04-08T11:40:55.979449Z","shell.execute_reply.started":"2025-04-08T11:40:55.975719Z","shell.execute_reply":"2025-04-08T11:40:55.978777Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"BOS_IDX = 1\nPAD_IDX = 0\nUNK_IDX = 2\nEOS_IDX = 21","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:55.980258Z","iopub.execute_input":"2025-04-08T11:40:55.980508Z","iopub.status.idle":"2025-04-08T11:40:55.993358Z","shell.execute_reply.started":"2025-04-08T11:40:55.980489Z","shell.execute_reply":"2025-04-08T11:40:55.992566Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"config = Config()\ntrainer = Trainer(config, dataloaders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:55.994251Z","iopub.execute_input":"2025-04-08T11:40:55.994480Z","iopub.status.idle":"2025-04-08T11:40:57.972269Z","shell.execute_reply.started":"2025-04-08T11:40:55.994449Z","shell.execute_reply":"2025-04-08T11:40:57.971632Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-22-825296cbe684>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:40:57.973022Z","iopub.execute_input":"2025-04-08T11:40:57.973240Z","iopub.status.idle":"2025-04-08T11:56:27.286813Z","shell.execute_reply.started":"2025-04-08T11:40:57.973219Z","shell.execute_reply":"2025-04-08T11:56:27.285922Z"}},"outputs":[{"name":"stderr","text":"[1/100] Train:   0%|          | 0/231 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n[1/100] Train: 100%|██████████| 231/231 [00:09<00:00, 23.99it/s, loss=1.04]\n[1/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 26.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.814773 from -1\n","output_type":"stream"},{"name":"stderr","text":"[2/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.96it/s, loss=0.642]\n[2/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8291174 from 0.814773\n","output_type":"stream"},{"name":"stderr","text":"[3/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.16it/s, loss=0.579]\n[3/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8474968 from 0.8291174\n","output_type":"stream"},{"name":"stderr","text":"[4/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.68it/s, loss=0.545]\n[4/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8519207 from 0.8474968\n","output_type":"stream"},{"name":"stderr","text":"[5/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.19it/s, loss=0.515]\n[5/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8610919 from 0.8519207\n","output_type":"stream"},{"name":"stderr","text":"[6/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.94it/s, loss=0.494]\n[6/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 36.21it/s]\n[7/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.62it/s, loss=0.49] \n[7/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8659865 from 0.8610919\n","output_type":"stream"},{"name":"stderr","text":"[8/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.16it/s, loss=0.469]\n[8/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8687012 from 0.8659865\n","output_type":"stream"},{"name":"stderr","text":"[9/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.05it/s, loss=0.458]\n[9/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.69it/s]\n[10/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.23it/s, loss=0.453]\n[10/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8731965 from 0.8687012\n","output_type":"stream"},{"name":"stderr","text":"[11/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.54it/s, loss=0.439]\n[11/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8763033 from 0.8731965\n","output_type":"stream"},{"name":"stderr","text":"[12/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.06it/s, loss=0.435]\n[12/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8785771 from 0.8763033\n","output_type":"stream"},{"name":"stderr","text":"[13/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.28it/s, loss=0.438]\n[13/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 30.02it/s]\n[14/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.06it/s, loss=0.423]\n[14/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.92it/s]\n[15/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.86it/s, loss=0.418]\n[15/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.05it/s]\n[16/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.09it/s, loss=0.416]\n[16/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8824905 from 0.8785771\n","output_type":"stream"},{"name":"stderr","text":"[17/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.17it/s, loss=0.407]\n[17/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.81it/s]\n[18/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.66it/s, loss=0.407]\n[18/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8854014 from 0.8824905\n","output_type":"stream"},{"name":"stderr","text":"[19/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.07it/s, loss=0.405]\n[19/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.58it/s]\n[20/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.11it/s, loss=0.401]\n[20/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8861566 from 0.8854014\n","output_type":"stream"},{"name":"stderr","text":"[21/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.63it/s, loss=0.397]\n[21/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8865182 from 0.8861566\n","output_type":"stream"},{"name":"stderr","text":"[22/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.13it/s, loss=0.394]\n[22/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.22it/s]\n[23/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.08it/s, loss=0.387]\n[23/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.96it/s]\n[24/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.11it/s, loss=0.378]\n[24/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.85it/s]\n[25/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.70it/s, loss=0.382]\n[25/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.50it/s]\n[26/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.46it/s, loss=0.376]\n[26/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8880329 from 0.8865182\n","output_type":"stream"},{"name":"stderr","text":"[27/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.23it/s, loss=0.374]\n[27/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 36.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8901149 from 0.8880329\n","output_type":"stream"},{"name":"stderr","text":"[28/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.38it/s, loss=0.371]\n[28/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.83it/s]\n[29/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.07it/s, loss=0.371]\n[29/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8904302 from 0.8901149\n","output_type":"stream"},{"name":"stderr","text":"[30/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.88it/s, loss=0.362]\n[30/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8925449 from 0.8904302\n","output_type":"stream"},{"name":"stderr","text":"[31/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.98it/s, loss=0.361]\n[31/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 30.82it/s]\n[32/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.98it/s, loss=0.362]\n[32/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8927173 from 0.8925449\n","output_type":"stream"},{"name":"stderr","text":"[33/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.07it/s, loss=0.359]\n[33/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.22it/s]\n[34/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.95it/s, loss=0.353]\n[34/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.39it/s]\n[35/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.56it/s, loss=0.351]\n[35/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8938555 from 0.8927173\n","output_type":"stream"},{"name":"stderr","text":"[36/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.29it/s, loss=0.349]\n[36/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.92it/s]\n[37/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.17it/s, loss=0.345]\n[37/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8952365 from 0.8938555\n","output_type":"stream"},{"name":"stderr","text":"[38/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.82it/s, loss=0.344]\n[38/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8953019 from 0.8952365\n","output_type":"stream"},{"name":"stderr","text":"[39/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.39it/s, loss=0.34] \n[39/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 32.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8963053 from 0.8953019\n","output_type":"stream"},{"name":"stderr","text":"[40/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.40it/s, loss=0.339]\n[40/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.59it/s]\n[41/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.01it/s, loss=0.339]\n[41/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.01it/s]\n[42/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.92it/s, loss=0.34] \n[42/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.40it/s]\n[43/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.29it/s, loss=0.335]\n[43/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8974132 from 0.8963053\n","output_type":"stream"},{"name":"stderr","text":"[44/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.13it/s, loss=0.332]\n[44/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.8983773 from 0.8974132\n","output_type":"stream"},{"name":"stderr","text":"[45/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.72it/s, loss=0.33] \n[45/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.38it/s]\n[46/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.83it/s, loss=0.326]\n[46/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.56it/s]\n[47/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.18it/s, loss=0.331]\n[47/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.19it/s]\n[48/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.21it/s, loss=0.323]\n[48/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.42it/s]\n[49/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.48it/s, loss=0.323]\n[49/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.899369 from 0.8983773\n","output_type":"stream"},{"name":"stderr","text":"[50/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.54it/s, loss=0.32] \n[50/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.42it/s]\n[51/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.76it/s, loss=0.32] \n[51/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.39it/s]\n[52/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.20it/s, loss=0.317]\n[52/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.60it/s]\n[53/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.88it/s, loss=0.314]\n[53/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 32.40it/s]\n[54/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.73it/s, loss=0.316]\n[54/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.900502 from 0.899369\n","output_type":"stream"},{"name":"stderr","text":"[55/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.53it/s, loss=0.312]\n[55/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.31it/s]\n[56/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.48it/s, loss=0.311]\n[56/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.86it/s]\n[57/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.85it/s, loss=0.312]\n[57/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9008622 from 0.900502\n","output_type":"stream"},{"name":"stderr","text":"[58/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.77it/s, loss=0.308]\n[58/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.50it/s]\n[59/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.13it/s, loss=0.306]\n[59/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.901219 from 0.9008622\n","output_type":"stream"},{"name":"stderr","text":"[60/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.57it/s, loss=0.301]\n[60/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.22it/s]\n[61/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.72it/s, loss=0.301]\n[61/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9013636 from 0.901219\n","output_type":"stream"},{"name":"stderr","text":"[62/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.15it/s, loss=0.302]\n[62/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9014002 from 0.9013636\n","output_type":"stream"},{"name":"stderr","text":"[63/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.60it/s, loss=0.299]\n[63/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9020313 from 0.9014002\n","output_type":"stream"},{"name":"stderr","text":"[64/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.04it/s, loss=0.299]\n[64/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.83it/s]\n[65/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.04it/s, loss=0.296]\n[65/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.77it/s]\n[66/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.51it/s, loss=0.294]\n[66/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9029605 from 0.9020313\n","output_type":"stream"},{"name":"stderr","text":"[67/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.90it/s, loss=0.294]\n[67/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.87it/s]\n[68/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.11it/s, loss=0.291]\n[68/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.87it/s]\n[69/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.64it/s, loss=0.288]\n[69/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9040429 from 0.9029605\n","output_type":"stream"},{"name":"stderr","text":"[70/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.94it/s, loss=0.287]\n[70/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.89it/s]\n[71/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.99it/s, loss=0.285]\n[71/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9052028 from 0.9040429\n","output_type":"stream"},{"name":"stderr","text":"[72/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.99it/s, loss=0.285]\n[72/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.17it/s]\n[73/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.62it/s, loss=0.281]\n[73/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.86it/s]\n[74/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.97it/s, loss=0.284]\n[74/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.60it/s]\n[75/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.13it/s, loss=0.281]\n[75/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.58it/s]\n[76/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.43it/s, loss=0.278]\n[76/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9054257 from 0.9052028\n","output_type":"stream"},{"name":"stderr","text":"[77/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.94it/s, loss=0.276]\n[77/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 36.19it/s]\n[78/100] Train: 100%|██████████| 231/231 [00:08<00:00, 26.10it/s, loss=0.273]\n[78/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.89it/s]\n[79/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.88it/s, loss=0.273]\n[79/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 32.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9066054 from 0.9054257\n","output_type":"stream"},{"name":"stderr","text":"[80/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.69it/s, loss=0.27] \n[80/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.01it/s]\n[81/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.50it/s, loss=0.271]\n[81/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.58it/s]\n[82/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.90it/s, loss=0.27] \n[82/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.27it/s]\n[83/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.47it/s, loss=0.266]\n[83/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.68it/s]\n[84/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.38it/s, loss=0.27] \n[84/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.77it/s]\n[85/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.97it/s, loss=0.264]\n[85/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.77it/s]\n[86/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.71it/s, loss=0.264]\n[86/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.79it/s]\n[87/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.92it/s, loss=0.258]\n[87/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9072825 from 0.9066054\n","output_type":"stream"},{"name":"stderr","text":"[88/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.92it/s, loss=0.262]\n[88/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9074697 from 0.9072825\n","output_type":"stream"},{"name":"stderr","text":"[89/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.97it/s, loss=0.258]\n[89/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.60it/s]\n[90/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.35it/s, loss=0.255]\n[90/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.82it/s]\n[91/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.82it/s, loss=0.254]\n[91/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9079218 from 0.9074697\n","output_type":"stream"},{"name":"stderr","text":"[92/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.96it/s, loss=0.257]\n[92/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"==> Best Accuracy improved to 0.9084729 from 0.9079218\n","output_type":"stream"},{"name":"stderr","text":"[93/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.64it/s, loss=0.253]\n[93/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.68it/s]\n[94/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.87it/s, loss=0.25] \n[94/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.18it/s]\n[95/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.89it/s, loss=0.247]\n[95/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 34.94it/s]\n[96/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.66it/s, loss=0.247]\n[96/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 33.61it/s]\n[97/100] Train: 100%|██████████| 231/231 [00:09<00:00, 25.47it/s, loss=0.246]\n[97/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 32.86it/s]\n[98/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.91it/s, loss=0.244]\n[98/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.01it/s]\n[99/100] Train: 100%|██████████| 231/231 [00:08<00:00, 25.78it/s, loss=0.241]\n[99/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.47it/s]\n[100/100] Train: 100%|██████████| 231/231 [00:09<00:00, 24.99it/s, loss=0.242]\n[100/100] Valid: 100%|██████████| 7/7 [00:00<00:00, 35.26it/s]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"len(tgt_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:56:27.287819Z","iopub.execute_input":"2025-04-08T11:56:27.288123Z","iopub.status.idle":"2025-04-08T11:56:27.293343Z","shell.execute_reply.started":"2025-04-08T11:56:27.288097Z","shell.execute_reply":"2025-04-08T11:56:27.292698Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"22"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.test_seq_acc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T11:56:27.294113Z","iopub.execute_input":"2025-04-08T11:56:27.294447Z","iopub.status.idle":"2025-04-08T11:58:34.916203Z","shell.execute_reply.started":"2025-04-08T11:56:27.294415Z","shell.execute_reply":"2025-04-08T11:58:34.915225Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-22-825296cbe684>:254: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(file, map_location=self.device)['state_dict']\n[100/100] Test: 100%|██████████| 786/786 [00:10<00:00, 76.63it/s]\n<ipython-input-21-096603f25c93>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(path)[\"state_dict\"])\n","output_type":"stream"},{"name":"stdout","text":"Calculating Sequence Accuracy for predictions\n","output_type":"stream"},{"name":"stderr","text":"Test: 100%|██████████| 786/786 [01:57<00:00,  6.71it/s]","output_type":"stream"},{"name":"stdout","text":"[ 1  7 20  7 11  8 13 13 13 14 17  6 19  7 20  8 13 13 13 13 14 17  5 18\n  7 20 12 17  4 19  3 19  8 20  7 21] [ 1  7 20  7 11  8 13 13 13 14 17  6 18  7 20  8 13 13 13 13 14 17  5 19\n  7 20 12 17  4 19  3 18  8 20  7 21]\nTest Accuracy: 0.7534554 | Valid Accuracy: 0.9084729\nTest Sequence Accuracy: 0.11323155216284987\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}