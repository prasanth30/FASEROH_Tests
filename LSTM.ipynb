{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d24ab9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007696,
     "end_time": "2025-04-08T07:21:18.141791",
     "exception": false,
     "start_time": "2025-04-08T07:21:18.134095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221ca288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:18.155010Z",
     "iopub.status.busy": "2025-04-08T07:21:18.154785Z",
     "iopub.status.idle": "2025-04-08T07:21:23.214769Z",
     "shell.execute_reply": "2025-04-08T07:21:23.213803Z"
    },
    "papermill": {
     "duration": 5.068251,
     "end_time": "2025-04-08T07:21:23.216436",
     "exception": false,
     "start_time": "2025-04-08T07:21:18.148185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q x-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a65e42",
   "metadata": {
    "papermill": {
     "duration": 0.00637,
     "end_time": "2025-04-08T07:21:23.230034",
     "exception": false,
     "start_time": "2025-04-08T07:21:23.223664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cbfdbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:23.250527Z",
     "iopub.status.busy": "2025-04-08T07:21:23.250163Z",
     "iopub.status.idle": "2025-04-08T07:21:28.990031Z",
     "shell.execute_reply": "2025-04-08T07:21:28.989277Z"
    },
    "papermill": {
     "duration": 5.752957,
     "end_time": "2025-04-08T07:21:28.991551",
     "exception": false,
     "start_time": "2025-04-08T07:21:23.238594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import sympy\n",
    "\n",
    "from sympy import expand\n",
    "from sympy import sympify\n",
    "from sympy import series\n",
    "from sympy import Symbol, symbols\n",
    "from sympy import im, I\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2831f6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:29.005829Z",
     "iopub.status.busy": "2025-04-08T07:21:29.005439Z",
     "iopub.status.idle": "2025-04-08T07:21:29.008867Z",
     "shell.execute_reply": "2025-04-08T07:21:29.008214Z"
    },
    "papermill": {
     "duration": 0.011741,
     "end_time": "2025-04-08T07:21:29.010108",
     "exception": false,
     "start_time": "2025-04-08T07:21:28.998367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOS_IDX = 0\n",
    "PAD_IDX = 1\n",
    "UNK_IDX = 2\n",
    "EOS_IDX = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731ab85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:29.023697Z",
     "iopub.status.busy": "2025-04-08T07:21:29.023451Z",
     "iopub.status.idle": "2025-04-08T07:21:29.032620Z",
     "shell.execute_reply": "2025-04-08T07:21:29.032037Z"
    },
    "papermill": {
     "duration": 0.017285,
     "end_time": "2025-04-08T07:21:29.033821",
     "exception": false,
     "start_time": "2025-04-08T07:21:29.016536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    experiment_name: Optional[str] = \"seq2seq_transformer\"\n",
    "    root_dir: Optional[str] = \"./\"\n",
    "    device: Optional[str] = \"cuda:0\"\n",
    "        \n",
    "    #training parameters\n",
    "    epochs: Optional[int] = 10\n",
    "    seed: Optional[int] = 42\n",
    "    use_half_precision: Optional[bool] = True\n",
    "\n",
    "    # scheduler parameters\n",
    "    scheduler_type: Optional[str] = \"cosine_annealing_warm_restart\" # multi_step or none\n",
    "    T_0: Optional[int] = 10\n",
    "    T_mult: Optional[int] = 1\n",
    "\n",
    "    # optimizer parameters\n",
    "    optimizer_type: Optional[str] = \"adam\" # sgd or adam\n",
    "    optimizer_lr: Optional[float] = 0.0005   \n",
    "    optimizer_momentum: Optional[float] = 0.9\n",
    "    optimizer_weight_decay: Optional[float] = 0.0001\n",
    "    optimizer_no_decay: Optional[list] = field(default_factory=list)\n",
    "    clip_grad_norm: Optional[float] = -1\n",
    "        \n",
    "    # Model Parameters\n",
    "    model_name: Optional[str] = \"seq2seq_transformer\"\n",
    "    hybrid: Optional[bool] = True\n",
    "    embedding_size: Optional[int] = 64\n",
    "    hidden_dim: Optional[int] = 64\n",
    "    pff_dim: Optional[int] = 512\n",
    "    nhead: Optional[int] = 8\n",
    "    num_encoder_layers: Optional[int] = 2\n",
    "    num_decoder_layers: Optional[int] = 6\n",
    "    dropout: Optional[int] = 0.2\n",
    "    pretrain: Optional[bool] = False\n",
    "    input_emb_size: Optional[int] = 64\n",
    "    max_input_points: Optional[int] = 210\n",
    "    src_vocab_size: Optional[int] = 32\n",
    "    tgt_vocab_size: Optional[int] = 22\n",
    "\n",
    "    # Criterion\n",
    "    criterion: Optional[str] = \"cross_entropy\"\n",
    "        \n",
    "    def print_config(self):\n",
    "        print(\"=\"*50+\"\\nConfig\\n\"+\"=\"*50)\n",
    "        for field in fields(self):\n",
    "            print(field.name.ljust(30), getattr(self, field.name))\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def save(self, root_dir):\n",
    "        path = root_dir + \"/config.txt\"\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"=\"*50+\"\\nConfig\\n\"+\"=\"*50 + \"\\n\")\n",
    "            for field in fields(self):\n",
    "                f.write(field.name.ljust(30) + \": \" + str(getattr(self, field.name)) + \"\\n\")\n",
    "            f.write(\"=\"*50)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9e5bf",
   "metadata": {
    "papermill": {
     "duration": 0.006297,
     "end_time": "2025-04-08T07:21:29.046661",
     "exception": false,
     "start_time": "2025-04-08T07:21:29.040364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f32154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:29.060202Z",
     "iopub.status.busy": "2025-04-08T07:21:29.060001Z",
     "iopub.status.idle": "2025-04-08T07:21:29.097097Z",
     "shell.execute_reply": "2025-04-08T07:21:29.096267Z"
    },
    "papermill": {
     "duration": 0.045274,
     "end_time": "2025-04-08T07:21:29.098405",
     "exception": false,
     "start_time": "2025-04-08T07:21:29.053131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pth = '/kaggle/input/data-no-dup/final_data_3372.csv'\n",
    "\n",
    "df = pd.read_csv(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed4a8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:29.112368Z",
     "iopub.status.busy": "2025-04-08T07:21:29.112164Z",
     "iopub.status.idle": "2025-04-08T07:21:29.116001Z",
     "shell.execute_reply": "2025-04-08T07:21:29.115382Z"
    },
    "papermill": {
     "duration": 0.012229,
     "end_time": "2025-04-08T07:21:29.117093",
     "exception": false,
     "start_time": "2025-04-08T07:21:29.104864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spt(i, order=4):\n",
    "    # print(df['expansion'].iloc[i])\n",
    "    # return df['expansion'].iloc[i].split('x**')\n",
    "    expr = sympify(df['expansion'].iloc[i]).evalf(4).as_poly()\n",
    "    # print(expr)\n",
    "    # print(expr.free_symbols)\n",
    "    coeffs = expr.all_coeffs()[::-1]\n",
    "    if len(coeffs) < order + 1:\n",
    "        coeffs += [0]*(order - len(coeffs) + 1)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657f0076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:29.131041Z",
     "iopub.status.busy": "2025-04-08T07:21:29.130839Z",
     "iopub.status.idle": "2025-04-08T07:21:40.270047Z",
     "shell.execute_reply": "2025-04-08T07:21:40.269125Z"
    },
    "papermill": {
     "duration": 11.147797,
     "end_time": "2025-04-08T07:21:40.271392",
     "exception": false,
     "start_time": "2025-04-08T07:21:29.123595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3372/3372 [00:11<00:00, 302.97it/s]\n"
     ]
    }
   ],
   "source": [
    "df_clean = pd.DataFrame(columns = df.columns)\n",
    "coeffs = []\n",
    "for idx, row in tqdm(df.iterrows(),total=len(df)):\n",
    "    try:\n",
    "        coeff = spt(idx)\n",
    "        coeffs.append(coeff)\n",
    "        df_clean.loc[len(df_clean)] = row\n",
    "    except Exception as ex:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb6a46f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:40.294751Z",
     "iopub.status.busy": "2025-04-08T07:21:40.294483Z",
     "iopub.status.idle": "2025-04-08T07:21:40.298619Z",
     "shell.execute_reply": "2025-04-08T07:21:40.297984Z"
    },
    "papermill": {
     "duration": 0.016936,
     "end_time": "2025-04-08T07:21:40.299867",
     "exception": false,
     "start_time": "2025-04-08T07:21:40.282931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean['coefficients'] = coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2edf764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:40.322195Z",
     "iopub.status.busy": "2025-04-08T07:21:40.321994Z",
     "iopub.status.idle": "2025-04-08T07:21:41.253539Z",
     "shell.execute_reply": "2025-04-08T07:21:41.252850Z"
    },
    "papermill": {
     "duration": 0.944312,
     "end_time": "2025-04-08T07:21:41.255097",
     "exception": false,
     "start_time": "2025-04-08T07:21:40.310785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "    val_size = val_ratio / (val_ratio + test_ratio)\n",
    "    val_data, test_data = train_test_split(temp_data, train_size=val_size, random_state=42)\n",
    "\n",
    "    data = {\n",
    "        'train': train_data,\n",
    "        'valid': val_data,\n",
    "        'test': test_data\n",
    "    }\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769966e",
   "metadata": {
    "papermill": {
     "duration": 0.010673,
     "end_time": "2025-04-08T07:21:41.277044",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.266371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05843898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.299805Z",
     "iopub.status.busy": "2025-04-08T07:21:41.299354Z",
     "iopub.status.idle": "2025-04-08T07:21:41.303249Z",
     "shell.execute_reply": "2025-04-08T07:21:41.302413Z"
    },
    "papermill": {
     "duration": 0.016456,
     "end_time": "2025-04-08T07:21:41.304571",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.288115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_exponent(expr: str) -> str:\n",
    "    # Step 1: Replace powers **2 to **4 with ^2 to ^4\n",
    "    expr = re.sub(r'(\\b\\w+\\b)\\s*\\*\\*\\s*([2-4])', r'<\\1^\\2>', expr)\n",
    " \n",
    "    # Step 2: Replace plain variables with variable_1 (not touching already transformed ones)\n",
    "    # Negative lookbehind for ^ or _, to avoid changing x^2 or x_1\n",
    "    expr = re.sub(r'(?<![\\^_])\\bx\\b(?![\\^_])', '<x^1>', expr)\n",
    "\n",
    "    return expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e788093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.326679Z",
     "iopub.status.busy": "2025-04-08T07:21:41.326435Z",
     "iopub.status.idle": "2025-04-08T07:21:41.332255Z",
     "shell.execute_reply": "2025-04-08T07:21:41.331693Z"
    },
    "papermill": {
     "duration": 0.018162,
     "end_time": "2025-04-08T07:21:41.333474",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.315312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dec_preproc(input_str, num_token='<NUM>'):\n",
    "    \n",
    "    exp = input_str\n",
    "    e = sympy.Symbol('e')\n",
    "    expr = str(sympify(exp).evalf(6,subs={'e':sympy.core.numbers.E})).replace(' ','')\n",
    "    expr = replace_exponent(expr)\n",
    "    expr_arr = expr.replace('+', ' + ').replace('*',' * ').replace('-',' - ').split(' ')\n",
    "\n",
    "    expr_mod = []\n",
    "\n",
    "    def check_float(f):\n",
    "        try:\n",
    "            _ = float(f)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    for i in expr_arr:\n",
    "        if i == '':\n",
    "            continue\n",
    "\n",
    "        if check_float(i) or check_float(i[:-1]):\n",
    "            for char in str(i):\n",
    "                expr_mod.append(char)\n",
    "        else:\n",
    "            expr_mod.append(i)\n",
    "    return expr_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abfc9ae",
   "metadata": {
    "papermill": {
     "duration": 0.010666,
     "end_time": "2025-04-08T07:21:41.354937",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.344271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf16d921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.377172Z",
     "iopub.status.busy": "2025-04-08T07:21:41.376947Z",
     "iopub.status.idle": "2025-04-08T07:21:41.380942Z",
     "shell.execute_reply": "2025-04-08T07:21:41.380153Z"
    },
    "papermill": {
     "duration": 0.016567,
     "end_time": "2025-04-08T07:21:41.382232",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.365665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_vocab = []\n",
    "\n",
    "src_vocab += ['<PAD>', '<SOS>', '<UNK>', 'x']\n",
    "src_vocab += [str(i) for i in range(10)]\n",
    "src_vocab += ['pi', 's+', 's-', 'E']\n",
    "src_vocab += ['add', 'mul', 'pow']\n",
    "src_vocab += ['sin', 'cos', 'tan', 'cot']\n",
    "src_vocab += ['asin', 'acos', 'atan', 'acot']\n",
    "src_vocab += ['ln', 'exp']\n",
    "src_vocab += ['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4058bb50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.404690Z",
     "iopub.status.busy": "2025-04-08T07:21:41.404444Z",
     "iopub.status.idle": "2025-04-08T07:21:41.408204Z",
     "shell.execute_reply": "2025-04-08T07:21:41.407561Z"
    },
    "papermill": {
     "duration": 0.015975,
     "end_time": "2025-04-08T07:21:41.409238",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.393263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_vocab = []\n",
    "\n",
    "tgt_vocab += ['<PAD>', '<SOS>', '<UNK>']\n",
    "tgt_vocab += [f'<x^{i}>' for i in range(1,5)]\n",
    "tgt_vocab += [str(i) for i in range(10)]\n",
    "tgt_vocab += ['*', '+', '-', '.']\n",
    "tgt_vocab += ['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c015b489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.431842Z",
     "iopub.status.busy": "2025-04-08T07:21:41.431584Z",
     "iopub.status.idle": "2025-04-08T07:21:41.436522Z",
     "shell.execute_reply": "2025-04-08T07:21:41.435913Z"
    },
    "papermill": {
     "duration": 0.017361,
     "end_time": "2025-04-08T07:21:41.437719",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.420358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_vocab), len(src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a2d247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.459912Z",
     "iopub.status.busy": "2025-04-08T07:21:41.459672Z",
     "iopub.status.idle": "2025-04-08T07:21:41.470029Z",
     "shell.execute_reply": "2025-04-08T07:21:41.469436Z"
    },
    "papermill": {
     "duration": 0.022733,
     "end_time": "2025-04-08T07:21:41.471224",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.448491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        # Initialize with default vocabulary if none provided\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Create mappings\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "        self.idx_to_token = {idx: token for idx, token in enumerate(self.vocab)}\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_idx = self.token_to_idx['<PAD>']\n",
    "        self.sos_idx = self.token_to_idx['<SOS>']\n",
    "        self.eos_idx = self.token_to_idx['<EOS>']\n",
    "        self.unk_idx = self.token_to_idx['<UNK>']\n",
    "    \n",
    "    def encode(self, tokens, add_special_tokens=True, max_length=None):\n",
    "        \"\"\"\n",
    "        Encode a list of tokens into indices\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): List of tokens to encode\n",
    "            add_special_tokens (bool): Whether to add SOS and EOS tokens\n",
    "            max_length (int, optional): Maximum length to pad/truncate to\n",
    "            \n",
    "        Returns:\n",
    "            list: List of token indices\n",
    "        \"\"\"\n",
    "        if add_special_tokens:\n",
    "            tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        indices = [self.token_to_idx.get(token, self.unk_idx) for token in tokens]\n",
    "        \n",
    "        # Handle padding/truncation if max_length specified\n",
    "        if max_length is not None:\n",
    "            if len(indices) < max_length:\n",
    "                # Pad sequence\n",
    "                indices += [self.pad_idx] * (max_length - len(indices))\n",
    "            else:\n",
    "                # Truncate sequence\n",
    "                indices = indices[:max_length]\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices, remove_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Decode a list of indices back into tokens\n",
    "        \n",
    "        Args:\n",
    "            indices (list): List of indices to decode\n",
    "            remove_special_tokens (bool): Whether to remove special tokens\n",
    "            \n",
    "        Returns:\n",
    "            list: List of decoded tokens\n",
    "        \"\"\"\n",
    "        # Convert indices to tokens\n",
    "        tokens = [self.idx_to_token.get(idx, '<UNK>') for idx in indices]\n",
    "        \n",
    "        # Remove special tokens if requested\n",
    "        if remove_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in ['<PAD>', '<SOS>', '<EOS>']]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def batch_encode(self, batch_tokens, add_special_tokens=True, max_length=None, return_tensors=False):\n",
    "        \"\"\"\n",
    "        Encode a batch of token lists\n",
    "        \n",
    "        Args:\n",
    "            batch_tokens (list): List of token lists to encode\n",
    "            add_special_tokens (bool): Whether to add SOS and EOS tokens\n",
    "            max_length (int, optional): Maximum length to pad/truncate to\n",
    "            return_tensors (bool): Whether to return PyTorch tensors\n",
    "            \n",
    "        Returns:\n",
    "            list or torch.Tensor: Batch of encoded sequences\n",
    "        \"\"\"\n",
    "        encoded_batch = [self.encode(tokens, add_special_tokens, max_length) for tokens in batch_tokens]\n",
    "        \n",
    "        # If max_length not specified, pad to the longest sequence in batch\n",
    "        if max_length is None and encoded_batch:\n",
    "            max_len = max(len(seq) for seq in encoded_batch)\n",
    "            encoded_batch = [seq + [self.pad_idx] * (max_len - len(seq)) for seq in encoded_batch]\n",
    "        \n",
    "        # Convert to tensors if requested\n",
    "        if return_tensors:\n",
    "            import torch\n",
    "            encoded_batch = torch.tensor(encoded_batch, dtype=torch.long)\n",
    "        \n",
    "        return encoded_batch\n",
    "    \n",
    "    def save_vocabulary(self, filepath):\n",
    "        \"\"\"Save vocabulary to a file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            for token in self.vocab:\n",
    "                f.write(f\"{token}\\n\")\n",
    "        print(f\"Vocabulary saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, filepath):\n",
    "        \"\"\"Load vocabulary from a file\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            vocab = [line.strip() for line in f]\n",
    "        return cls(vocab)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary\"\"\"\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def add_tokens(self, new_tokens):\n",
    "        \"\"\"\n",
    "        Add new tokens to the vocabulary\n",
    "        \n",
    "        Args:\n",
    "            new_tokens (list): List of tokens to add\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of tokens added\n",
    "        \"\"\"\n",
    "        tokens_added = 0\n",
    "        for token in new_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab.append(token)\n",
    "                self.token_to_idx[token] = len(self.vocab) - 1\n",
    "                self.idx_to_token[len(self.vocab) - 1] = token\n",
    "                tokens_added += 1\n",
    "        \n",
    "        return tokens_added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b4839",
   "metadata": {
    "papermill": {
     "duration": 0.010507,
     "end_time": "2025-04-08T07:21:41.492504",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.481997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e746bc25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.514870Z",
     "iopub.status.busy": "2025-04-08T07:21:41.514620Z",
     "iopub.status.idle": "2025-04-08T07:21:41.519950Z",
     "shell.execute_reply": "2025-04-08T07:21:41.519357Z"
    },
    "papermill": {
     "duration": 0.017801,
     "end_time": "2025-04-08T07:21:41.521133",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.503332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Taylor_data(Dataset):\n",
    "    \n",
    "    def __init__(self, df, src_vocab, tgt_vocab):\n",
    "        self.enc_tokenizer = Tokenizer(src_vocab)\n",
    "        self.dec_tokenizer = Tokenizer(tgt_vocab)\n",
    "\n",
    "        self.df = df\n",
    "        \n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        self.build_dataset()\n",
    "    \n",
    "    def build_dataset(self):\n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            if len(row['prefix']) > 200:\n",
    "                continue\n",
    "                \n",
    "            tgt_preproc = dec_preproc(row['expansion'])\n",
    "            src_ids = self.enc_tokenizer.encode(eval(row['prefix']))\n",
    "            tgt_ids = self.dec_tokenizer.encode(tgt_preproc)\n",
    "            \n",
    "            self.src.append(src_ids)\n",
    "            self.tgt.append(tgt_ids)\n",
    "        \n",
    "        print('Built Dataset')\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return torch.tensor(self.src[idx]).long(), torch.tensor(self.tgt[idx]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47343799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.543092Z",
     "iopub.status.busy": "2025-04-08T07:21:41.542896Z",
     "iopub.status.idle": "2025-04-08T07:21:41.546539Z",
     "shell.execute_reply": "2025-04-08T07:21:41.545933Z"
    },
    "papermill": {
     "duration": 0.015851,
     "end_time": "2025-04-08T07:21:41.547620",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.531769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch, num_batch = [], [], []\n",
    "    for (src_sample, tgt_sample) in batch:\n",
    "        \n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "    \n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8c038eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.569967Z",
     "iopub.status.busy": "2025-04-08T07:21:41.569760Z",
     "iopub.status.idle": "2025-04-08T07:21:41.573809Z",
     "shell.execute_reply": "2025-04-08T07:21:41.573147Z"
    },
    "papermill": {
     "duration": 0.016393,
     "end_time": "2025-04-08T07:21:41.574980",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.558587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(datasets, train_bs, val_bs, test_bs):\n",
    "    \"\"\"\n",
    "    Get data loaders for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "    - datasets: Dictionary containing train, validation, and test datasets\n",
    "    - train_bs: Batch size for training\n",
    "    - val_bs: Batch size for validation\n",
    "    - test_bs: Batch size for testing\n",
    "\n",
    "    Returns:\n",
    "    - dataloaders: Dictionary containing train, validation, and test data loaders\n",
    "    \"\"\"\n",
    "    train_dataloader = DataLoader(datasets['train'], batch_size=train_bs,\n",
    "                                  shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(datasets['valid'], batch_size=val_bs,\n",
    "                                  shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(datasets['test'], batch_size=test_bs,\n",
    "                                  shuffle=False, num_workers=2, pin_memory=False, collate_fn=collate_fn)\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\":train_dataloader,\n",
    "        \"test\":test_dataloader,\n",
    "        \"valid\":val_dataloader\n",
    "        }\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d919fa",
   "metadata": {
    "papermill": {
     "duration": 0.010495,
     "end_time": "2025-04-08T07:21:41.596313",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.585818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58f041b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.618422Z",
     "iopub.status.busy": "2025-04-08T07:21:41.618221Z",
     "iopub.status.idle": "2025-04-08T07:21:41.625698Z",
     "shell.execute_reply": "2025-04-08T07:21:41.625097Z"
    },
    "papermill": {
     "duration": 0.019936,
     "end_time": "2025-04-08T07:21:41.626912",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.606976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt, device):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (torch.zeros((src.shape[0], src_seq_len), device=device)).type(torch.bool)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX)\n",
    "    tgt_mask = tgt_mask\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def sequence_accuracy(y_pred, y_true):\n",
    "\n",
    "    count = 0\n",
    "    total = len(y_pred)\n",
    "    for (predicted_tokens, original_tokens) in zip(y_pred, y_true):\n",
    "        original_tokens = original_tokens.tolist()\n",
    "        predicted_tokens = predicted_tokens.tolist()\n",
    "        if original_tokens == predicted_tokens:\n",
    "            count = count+1\n",
    "\n",
    "    return count/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261aef1",
   "metadata": {
    "papermill": {
     "duration": 0.010509,
     "end_time": "2025-04-08T07:21:41.648313",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.637804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99f54b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.670489Z",
     "iopub.status.busy": "2025-04-08T07:21:41.670284Z",
     "iopub.status.idle": "2025-04-08T07:21:41.684089Z",
     "shell.execute_reply": "2025-04-08T07:21:41.683489Z"
    },
    "papermill": {
     "duration": 0.026188,
     "end_time": "2025-04-08T07:21:41.685196",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.659008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "# https://github.com/neerajanand321/SYMBA_Pytorch/blob/main/models/seq2seq_transformer.py\n",
    "class TokenEmbedding(nn.Module):\n",
    "    ''' helper Module to convert tensor of input indices into corresponding tensor of token embeddings'''\n",
    "    \n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    ''' helper Module that adds positional encoding to the token embedding to introduce a notion of word order.'''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''Seq2Seq Network'''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 input_emb_size: int,\n",
    "                 max_input_points: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1,):\n",
    "        super(Model, self).__init__()\n",
    "        # self.transformer = Transformer(d_model=emb_size,\n",
    "        #                                nhead=nhead,\n",
    "        #                                num_encoder_layers=num_encoder_layers,\n",
    "        #                                num_decoder_layers=num_decoder_layers,\n",
    "        #                                dim_feedforward=dim_feedforward,\n",
    "        #                                dropout=dropout,\n",
    "        #                                batch_first=True)\n",
    "        self.encoder = nn.LSTM(input_size = emb_size,\n",
    "                                hidden_size = dim_feedforward,\n",
    "                                num_layers = num_encoder_layers, \n",
    "                                batch_first = True, \n",
    "                                dropout = dropout,\n",
    "                                proj_size = emb_size\n",
    "                        )\n",
    "        self.decoder = nn.LSTM(input_size = emb_size,\n",
    "                                hidden_size = dim_feedforward,\n",
    "                                num_layers = num_encoder_layers, \n",
    "                                batch_first = True, \n",
    "                                dropout = dropout,\n",
    "                               proj_size = emb_size\n",
    "                              )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor,\n",
    "               teacher_forcing_ratio = 0.9):\n",
    "        \n",
    "        src_emb = self.src_tok_emb(src)\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        target_length = trg.shape[1]\n",
    "        target_vocab_size = self.generator.out_features\n",
    "        \n",
    "        # Initialize tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, target_length, target_vocab_size).to(trg.device)\n",
    "        # Encode the source sequence\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        _, (hidden, cell) = self.encoder(src_emb)\n",
    "        \n",
    "        # First decoder input is the first token of target sequence (SOS token)\n",
    "        decoder_input = trg[:, 0:1]\n",
    "        \n",
    "        # Teacher forcing implementation\n",
    "        for t in range(1, target_length):\n",
    "            # Get embedding for current input\n",
    "            decoder_input_emb = self.positional_encoding(self.tgt_tok_emb(decoder_input))\n",
    "\n",
    "            # Pass through decoder - keeping batch dimension intact\n",
    "            decoder_output, (hidden, cell) = self.decoder(decoder_input_emb, (hidden, cell))\n",
    "            # Generate prediction\n",
    "            prediction = self.generator(decoder_output)\n",
    "            \n",
    "            # Store prediction for this timestep\n",
    "            outputs[:, t-1, :] = prediction.squeeze(1)\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            \n",
    "            if use_teacher_forcing:\n",
    "                # Use actual next token from target\n",
    "                decoder_input = trg[:, t:t+1]\n",
    "            else:\n",
    "                # Use highest probability predicted token\n",
    "                topv, topi = prediction.squeeze(1).topk(1)\n",
    "                decoder_input = topi.detach()  # detach from history as input\n",
    "                \n",
    "        # Handle last prediction\n",
    "        decoder_input_emb = self.positional_encoding(self.tgt_tok_emb(decoder_input))\n",
    "        decoder_output, _ = self.decoder(decoder_input_emb, (hidden, cell))\n",
    "        prediction = self.generator(decoder_output)\n",
    "        outputs[:, target_length-1, :] = prediction.squeeze(1)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        # outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "        #                         src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        # return self.generator(outs)\n",
    "    def encode(self, src: Tensor, src_mask: Tensor = None):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        return self.encoder(src_emb)\n",
    "    \n",
    "    def decode(self, tgt: Tensor, memory: Tensor = None, tgt_mask: Tensor = None):\n",
    "        \"\"\"\n",
    "        Decode function for inference\n",
    "        memory should be a tuple of (hidden_state, cell_state) from encoder\n",
    "        \"\"\"\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "            \n",
    "        if isinstance(memory, tuple) and len(memory) == 2:\n",
    "            # If memory is provided as (hidden, cell) tuple\n",
    "            hidden_state, cell_state = memory\n",
    "            return self.decoder(tgt_emb, (hidden_state, cell_state))\n",
    "        else:\n",
    "            # Fallback if memory isn't properly formatted\n",
    "            return self.decoder(tgt_emb)\n",
    "\n",
    "    # def encode(self, src: Tensor, src_mask: Tensor):\n",
    "    #     return self.encoder(self.src_tok_emb(src), src_mask)\n",
    "\n",
    "    # def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "    #     return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7df09",
   "metadata": {
    "papermill": {
     "duration": 0.010583,
     "end_time": "2025-04-08T07:21:41.706525",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.695942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed49035b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.728581Z",
     "iopub.status.busy": "2025-04-08T07:21:41.728379Z",
     "iopub.status.idle": "2025-04-08T07:21:41.739831Z",
     "shell.execute_reply": "2025-04-08T07:21:41.739161Z"
    },
    "papermill": {
     "duration": 0.023895,
     "end_time": "2025-04-08T07:21:41.741024",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.717129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"\n",
    "    Predictor class for generating predictions using a trained model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize Predictor object.\n",
    "\n",
    "        Args:\n",
    "        - config: Configuration object containing model parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "\n",
    "        # Get the model\n",
    "        self.model = self.get_model()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Load the best checkpoint\n",
    "        self.logs_dir = os.path.join(self.config.root_dir, self.config.experiment_name)\n",
    "        path = os.path.join(self.logs_dir, \"best_checkpoint.pth\")\n",
    "        self.model.load_state_dict(torch.load(path)[\"state_dict\"])\n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_model(self):\n",
    "        if self.config.model_name == \"seq2seq_transformer\":\n",
    "            model = Model(num_encoder_layers=self.config.num_encoder_layers,\n",
    "                          num_decoder_layers=self.config.num_decoder_layers,\n",
    "                          emb_size=self.config.embedding_size,\n",
    "                          nhead=self.config.nhead,\n",
    "                          src_vocab_size=self.config.src_vocab_size,\n",
    "                          tgt_vocab_size=self.config.tgt_vocab_size,\n",
    "                          input_emb_size=self.config.input_emb_size,\n",
    "                          max_input_points=self.config.max_input_points,\n",
    "                          )\n",
    "\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def greedy_decode(self, src, src_mask, max_len, start_symbol, src_padding_mask=None):\n",
    "        src = src.to(self.device)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src_padding_mask = src_padding_mask.to(self.device)\n",
    "        dim = 1\n",
    "\n",
    "        memory = self.model.encode(src, src_mask)\n",
    "        memory = memory.to(self.device)\n",
    "        dim = 1\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(self.device)\n",
    "        for i in range(max_len-1):\n",
    "\n",
    "            tgt_mask = (self.generate_square_subsequent_mask(ys.size(1), self.device).type(torch.bool)).to(self.device)\n",
    "\n",
    "            out = self.model.decode(ys, memory, tgt_mask)\n",
    "            prob = self.model.generator(out[:, -1])\n",
    "\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=dim)\n",
    "            if next_word == EOS_IDX:\n",
    "                break\n",
    "\n",
    "        return ys\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        \n",
    "        if self.config.model_name == \"seq2seq_transformer\":\n",
    "            src = x\n",
    "            num_tokens = src.shape[1]\n",
    "\n",
    "            src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "            src_padding_mask = torch.zeros(1, num_tokens).type(torch.bool)\n",
    "            tgt_tokens = self.greedy_decode(src, src_mask, max_len=256, start_symbol=BOS_IDX, src_padding_mask=src_padding_mask).flatten()\n",
    "\n",
    "            return tgt_tokens\n",
    "        else:\n",
    "            ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(self.device)\n",
    "            e_mask = torch.zeros(1, x.shape[1]).type(torch.bool).to(self.device)\n",
    "            memory = self.model.encoder(x, e_mask)\n",
    "\n",
    "            for idx in range(1, 256):\n",
    "                d_mask = torch.triu(torch.full((ys.size(1), ys.size(1)), float('-inf')), diagonal=1).to(self.device)\n",
    "                d_out = self.model.decoder(ys, memory, e_mask, d_mask)\n",
    "\n",
    "                prob = self.model.generator(d_out[:, -1])\n",
    "                _, next_word = torch.max(prob, dim=1)\n",
    "                next_word = next_word.item()\n",
    "                ys = torch.cat([ys, torch.ones(1, 1).type_as(x.data).fill_(next_word)], dim=1)\n",
    "                if next_word == EOS_IDX:\n",
    "                    break\n",
    "\n",
    "            return ys.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babe399",
   "metadata": {
    "papermill": {
     "duration": 0.010504,
     "end_time": "2025-04-08T07:21:41.762219",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.751715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25b5d48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.784564Z",
     "iopub.status.busy": "2025-04-08T07:21:41.784359Z",
     "iopub.status.idle": "2025-04-08T07:21:41.808490Z",
     "shell.execute_reply": "2025-04-08T07:21:41.807947Z"
    },
    "papermill": {
     "duration": 0.036902,
     "end_time": "2025-04-08T07:21:41.809804",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.772902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for training and evaluating a PyTorch model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, dataloaders):\n",
    "        \"\"\"\n",
    "        Initialize Trainer object.\n",
    "\n",
    "        Args:\n",
    "        - config: Configuration object containing training parameters\n",
    "        - dataloaders: Dictionary containing data loaders for train, validation, and test sets\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "        seed_everything(self.config.seed)\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        if self.config.use_half_precision:\n",
    "            self.dtype = torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        # Initialize model, optimizer, scheduler, and criterion\n",
    "        self.model = self.get_model()\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = self.get_optimizer()\n",
    "        self.scheduler = self.get_scheduler()\n",
    "        self.criterion = self.get_criterion()\n",
    "\n",
    "        # Initialize training-related variables\n",
    "        self.current_epoch = 0\n",
    "        self.best_accuracy = -1\n",
    "        self.best_val_loss = 1e6\n",
    "        self.train_loss_list = []\n",
    "        self.valid_loss_list = []\n",
    "        self.valid_accuracy_tok_list = []\n",
    "\n",
    "        # Create directory for saving logs\n",
    "        self.logs_dir = os.path.join(self.config.root_dir, self.config.experiment_name)\n",
    "        os.makedirs(self.logs_dir, exist_ok=True)\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the model based on the configuration.\n",
    "        \"\"\"\n",
    "        if self.config.model_name == \"seq2seq_transformer\":\n",
    "            model = Model(num_encoder_layers=self.config.num_encoder_layers,\n",
    "                          num_decoder_layers=self.config.num_decoder_layers,\n",
    "                          emb_size=self.config.embedding_size,\n",
    "                          nhead=self.config.nhead,\n",
    "                          src_vocab_size=self.config.src_vocab_size,\n",
    "                          tgt_vocab_size=self.config.tgt_vocab_size,\n",
    "                          input_emb_size=self.config.input_emb_size,\n",
    "                          max_input_points=self.config.max_input_points,\n",
    "                          )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the optimizer based on the configuration.\n",
    "        \"\"\"\n",
    "        optimizer_parameters = self.model.parameters()\n",
    "\n",
    "        if self.config.optimizer_type == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(optimizer_parameters, lr=self.config.optimizer_lr, momentum=self.config.optimizer_momentum,)\n",
    "        elif self.config.optimizer_type == \"adam\":\n",
    "            optimizer = torch.optim.Adam(optimizer_parameters, lr=self.config.optimizer_lr, eps=1e-8, weight_decay=self.config.optimizer_weight_decay)\n",
    "        elif self.config.optimizer_type == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(optimizer_parameters, lr=self.config.optimizer_lr, eps=1e-8, weight_decay=self.config.optimizer_weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def get_scheduler(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the learning rate scheduler based on the configuration.\n",
    "        \"\"\"\n",
    "        if self.config.scheduler_type == \"multi_step\":\n",
    "            scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=self.config.scheduler_milestones, gamma=self.config.scheduler_gamma)\n",
    "        elif self.config.scheduler_type == \"reduce_lr_on_plateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=2)\n",
    "        elif self.config.scheduler_type == \"cosine_annealing_warm_restart\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, self.config.T_0, self.config.T_mult)\n",
    "        elif self.config.scheduler_type == \"none\":\n",
    "            scheduler = None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return scheduler\n",
    "\n",
    "    \n",
    "    def get_criterion(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the loss function based on the configuration.\n",
    "        \"\"\"\n",
    "        if self.config.criterion == \"cross_entropy\":\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return criterion\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        pbar = tqdm(self.dataloaders['train'], total=len(self.dataloaders['train']))\n",
    "        pbar.set_description(f\"[{self.current_epoch+1}/{self.config.epochs}] Train\")\n",
    "        running_loss = AverageMeter()\n",
    "        for src, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "\n",
    "            bs = src.size(0)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "                if self.config.model_name == \"seq2seq_transformer\":\n",
    "                    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:, :-1], self.device)\n",
    "                    logits = self.model(src, tgt[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "                    loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "                else:\n",
    "                    logits = self.model(src, tgt[:, :-1])\n",
    "                    loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "                \n",
    "            running_loss.update(loss.item(), bs)\n",
    "            pbar.set_postfix(loss=running_loss.avg)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "\n",
    "            if self.config.clip_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_grad_norm)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "        return running_loss.avg\n",
    "\n",
    "    def evaluate(self, phase):\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation or test data.\n",
    "\n",
    "        Args:\n",
    "        - phase: Phase of evaluation, either \"valid\" or \"test\".\n",
    "\n",
    "        Returns:\n",
    "        - Tuple containing average token accuracy and average loss.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        pbar = tqdm(self.dataloaders[phase], total=len(self.dataloaders[phase]))\n",
    "        pbar.set_description(f\"[{self.current_epoch+1}/{self.config.epochs}] {phase.capitalize()}\")\n",
    "        running_loss = AverageMeter()\n",
    "        running_acc_tok = AverageMeter()\n",
    "        \n",
    "        \n",
    "        for src, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "            bs = src.size(0)\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "                if self.config.model_name == \"seq2seq_transformer\":\n",
    "                    with torch.no_grad():\n",
    "                        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:, :-1], self.device)\n",
    "                        logits = self.model(src, tgt[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "                        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(src, tgt[:, :-1])\n",
    "                        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "            y_pred = torch.argmax(logits.reshape(-1, logits.shape[-1]), 1)\n",
    "            correct = (y_pred == tgt[:, 1:].reshape(-1)).cpu().numpy().mean()\n",
    "            \n",
    "            running_loss.update(loss.item(), bs)\n",
    "            running_acc_tok.update(correct, bs)\n",
    "            \n",
    "        return running_acc_tok.avg, running_loss.avg\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \"\"\"\n",
    "        start_epoch = self.current_epoch\n",
    "        for self.current_epoch in range(start_epoch, self.config.epochs):\n",
    "            training_loss = self.train_one_epoch() \n",
    "            valid_accuracy_tok, valid_loss = self.evaluate(\"valid\")\n",
    "            \n",
    "            self.train_loss_list.append(round(training_loss, 7))\n",
    "            self.valid_loss_list.append(round(valid_loss, 7))\n",
    "            self.valid_accuracy_tok_list.append(round(valid_accuracy_tok, 7))\n",
    "            \n",
    "            if self.scheduler == \"multi_step\":\n",
    "                self.scheduler.step()\n",
    "            elif self.scheduler == \"reduce_lr_on_plateau\":\n",
    "                self.scheduler.step(valid_loss)\n",
    "                \n",
    "            if valid_loss<self.best_val_loss:\n",
    "                self.best_val_loss = valid_loss\n",
    "\n",
    "            self.save_model(\"last_checkpoint.pth\")\n",
    "\n",
    "            if valid_accuracy_tok > self.best_accuracy:\n",
    "                print(f\"==> Best Accuracy improved to {round(valid_accuracy_tok, 7)} from {self.best_accuracy}\")\n",
    "                self.best_accuracy = round(valid_accuracy_tok, 7)\n",
    "                self.save_model(\"best_checkpoint.pth\")\n",
    "            \n",
    "            self.log_results()\n",
    "\n",
    "        \n",
    "    def save_model(self, file_name):\n",
    "        \"\"\"\n",
    "        Save model checkpoints.\n",
    "        \"\"\"\n",
    "        state_dict = self.model.state_dict()\n",
    "        torch.save({\n",
    "                \"epoch\": self.current_epoch + 1,\n",
    "                \"state_dict\": state_dict,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                \"train_loss_list\": self.train_loss_list,\n",
    "                \"valid_loss_list\": self.valid_loss_list,\n",
    "                \"valid_accuracy_tok_list\": self.valid_accuracy_tok_list,\n",
    "            }, os.path.join(self.logs_dir, file_name))\n",
    "\n",
    "    def log_results(self):\n",
    "        \"\"\"\n",
    "        Log training results to a CSV file.\n",
    "        \"\"\"\n",
    "        data_list = [self.train_loss_list, self.valid_loss_list, self.valid_accuracy_tok_list]\n",
    "        column_list = ['train_losses', 'valid_losses', 'token_valid_accuracy']\n",
    "        \n",
    "        df_data = np.array(data_list).T\n",
    "        df = pd.DataFrame(df_data, columns=column_list)\n",
    "        df.to_csv(os.path.join(self.logs_dir, \"logs.csv\"))\n",
    "        \n",
    "    def test_seq_acc(self):\n",
    "        \"\"\"\n",
    "        Evaluate model's sequence accuracy on test data.\n",
    "        \"\"\"\n",
    "        file = os.path.join(self.logs_dir, \"best_checkpoint.pth\")\n",
    "        state_dict = torch.load(file, map_location=self.device)['state_dict']\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        \n",
    "        test_accuracy_tok, _ = self.evaluate(\"test\")\n",
    "        \n",
    "        predictor = Predictor(self.config)\n",
    "        \n",
    "        print(\"Calculating Sequence Accuracy for predictions (1 example per batch)\")\n",
    "        pbar = tqdm(self.dataloaders[\"test\"], total=len(self.dataloaders[\"test\"]))\n",
    "        pbar.set_description(f\"Test\")\n",
    "        \n",
    "        y_preds = []\n",
    "        y_true = []\n",
    "        for src, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.numpy()\n",
    "            bs = src.size(0)\n",
    "            y_pred = predictor.predict(src[0].unsqueeze(0)) #only one example from each batch\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            y_true.append(np.trim_zeros(tgt[0]))\n",
    "        print(y_preds[1], y_true[1])\n",
    "        test_accuracy_seq = sequence_accuracy(y_true, y_preds)\n",
    "        f= open(os.path.join(self.logs_dir, \"score.txt\"),\"w+\")\n",
    "        f.write(f\"Token Accuracy = {(round(test_accuracy_tok, 7))}\\n\")\n",
    "        f.write(f\"Sequence Accuracy = {(round(test_accuracy_seq, 7))}\\n\")\n",
    "        f.close()\n",
    "        print(f\"Test Accuracy: {round(test_accuracy_tok, 7)} | Valid Accuracy: {self.best_accuracy}\") \n",
    "        print(f\"Test Sequence Accuracy: {test_accuracy_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd184da",
   "metadata": {
    "papermill": {
     "duration": 0.010644,
     "end_time": "2025-04-08T07:21:41.831401",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.820757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db990b3",
   "metadata": {
    "papermill": {
     "duration": 0.010572,
     "end_time": "2025-04-08T07:21:41.852520",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.841948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eba67d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:41.874551Z",
     "iopub.status.busy": "2025-04-08T07:21:41.874346Z",
     "iopub.status.idle": "2025-04-08T07:21:49.684315Z",
     "shell.execute_reply": "2025-04-08T07:21:49.683342Z"
    },
    "papermill": {
     "duration": 7.822614,
     "end_time": "2025-04-08T07:21:49.685761",
     "exception": false,
     "start_time": "2025-04-08T07:21:41.863147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2132/2132 [00:05<00:00, 401.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 457/457 [00:01<00:00, 344.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 457/457 [00:01<00:00, 394.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "df_train, df_valid, df_test = split_data(df_clean)\n",
    "datasets = {\n",
    "    'train': Taylor_data(df_train, src_vocab, tgt_vocab),\n",
    "    'valid': Taylor_data(df_valid, src_vocab, tgt_vocab),\n",
    "    'test': Taylor_data(df_test, src_vocab, tgt_vocab)\n",
    "}\n",
    "# dataloaders = get_dataloaders(datasets, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58c16486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:49.715435Z",
     "iopub.status.busy": "2025-04-08T07:21:49.715177Z",
     "iopub.status.idle": "2025-04-08T07:21:49.718778Z",
     "shell.execute_reply": "2025-04-08T07:21:49.717989Z"
    },
    "papermill": {
     "duration": 0.019563,
     "end_time": "2025-04-08T07:21:49.720093",
     "exception": false,
     "start_time": "2025-04-08T07:21:49.700530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders = get_dataloaders(datasets, 128, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbd2d810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:49.749314Z",
     "iopub.status.busy": "2025-04-08T07:21:49.749082Z",
     "iopub.status.idle": "2025-04-08T07:21:49.752354Z",
     "shell.execute_reply": "2025-04-08T07:21:49.751548Z"
    },
    "papermill": {
     "duration": 0.01915,
     "end_time": "2025-04-08T07:21:49.753473",
     "exception": false,
     "start_time": "2025-04-08T07:21:49.734323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOS_IDX = 1\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 2\n",
    "EOS_IDX = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc9993bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:49.785958Z",
     "iopub.status.busy": "2025-04-08T07:21:49.785717Z",
     "iopub.status.idle": "2025-04-08T07:21:51.603039Z",
     "shell.execute_reply": "2025-04-08T07:21:51.602109Z"
    },
    "papermill": {
     "duration": 1.836839,
     "end_time": "2025-04-08T07:21:51.604547",
     "exception": false,
     "start_time": "2025-04-08T07:21:49.767708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7948be0243d7>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "trainer = Trainer(config, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88a14685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:21:51.634197Z",
     "iopub.status.busy": "2025-04-08T07:21:51.633937Z",
     "iopub.status.idle": "2025-04-08T07:22:24.221805Z",
     "shell.execute_reply": "2025-04-08T07:22:24.220608Z"
    },
    "papermill": {
     "duration": 32.603779,
     "end_time": "2025-04-08T07:22:24.223111",
     "exception": false,
     "start_time": "2025-04-08T07:21:51.619332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/10] Train: 100%|██████████| 16/16 [00:03<00:00,  4.82it/s, loss=2.08]\n",
      "[1/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.6853112 from -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.00it/s, loss=1.05]\n",
      "[2/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7064019 from 0.6853112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.20it/s, loss=0.985]\n",
      "[3/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7263837 from 0.7064019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.18it/s, loss=0.941]\n",
      "[4/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7287521 from 0.7263837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s, loss=0.914]\n",
      "[5/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7516313 from 0.7287521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.31it/s, loss=0.857]\n",
      "[6/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7691119 from 0.7516313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.15it/s, loss=0.765]\n",
      "[7/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7994352 from 0.7691119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.39it/s, loss=0.691]\n",
      "[8/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.8224497 from 0.7994352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.40it/s, loss=0.617]\n",
      "[9/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.8365121 from 0.8224497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/10] Train: 100%|██████████| 16/16 [00:02<00:00,  6.12it/s, loss=0.578]\n",
      "[10/10] Valid: 100%|██████████| 7/7 [00:00<00:00, 13.46it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9260e",
   "metadata": {
    "papermill": {
     "duration": 0.033288,
     "end_time": "2025-04-08T07:22:24.291223",
     "exception": false,
     "start_time": "2025-04-08T07:22:24.257935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7057290,
     "sourceId": 11287173,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7081499,
     "sourceId": 11322065,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 70.46968,
   "end_time": "2025-04-08T07:22:26.047425",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-08T07:21:15.577745",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
