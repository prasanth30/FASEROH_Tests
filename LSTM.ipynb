{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e4345",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007107,
     "end_time": "2025-04-06T20:01:56.397815",
     "exception": false,
     "start_time": "2025-04-06T20:01:56.390708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289a2fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:01:56.411637Z",
     "iopub.status.busy": "2025-04-06T20:01:56.411315Z",
     "iopub.status.idle": "2025-04-06T20:02:01.637635Z",
     "shell.execute_reply": "2025-04-06T20:02:01.636769Z"
    },
    "papermill": {
     "duration": 5.23487,
     "end_time": "2025-04-06T20:02:01.639257",
     "exception": false,
     "start_time": "2025-04-06T20:01:56.404387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q x-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91885ba6",
   "metadata": {
    "papermill": {
     "duration": 0.006423,
     "end_time": "2025-04-06T20:02:01.652731",
     "exception": false,
     "start_time": "2025-04-06T20:02:01.646308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fc8d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:01.666772Z",
     "iopub.status.busy": "2025-04-06T20:02:01.666522Z",
     "iopub.status.idle": "2025-04-06T20:02:07.492418Z",
     "shell.execute_reply": "2025-04-06T20:02:07.491687Z"
    },
    "papermill": {
     "duration": 5.834616,
     "end_time": "2025-04-06T20:02:07.493967",
     "exception": false,
     "start_time": "2025-04-06T20:02:01.659351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass, field, fields\n",
    "from typing import List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sympy\n",
    "from sympy import expand\n",
    "from sympy import sympify\n",
    "from sympy import series\n",
    "from sympy import Symbol, symbols\n",
    "from sympy import im, I\n",
    "\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef19491a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:07.509154Z",
     "iopub.status.busy": "2025-04-06T20:02:07.508739Z",
     "iopub.status.idle": "2025-04-06T20:02:07.512119Z",
     "shell.execute_reply": "2025-04-06T20:02:07.511469Z"
    },
    "papermill": {
     "duration": 0.012168,
     "end_time": "2025-04-06T20:02:07.513334",
     "exception": false,
     "start_time": "2025-04-06T20:02:07.501166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOS_IDX = 1\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 2\n",
    "EOS_IDX = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670285f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:07.527134Z",
     "iopub.status.busy": "2025-04-06T20:02:07.526896Z",
     "iopub.status.idle": "2025-04-06T20:02:07.536230Z",
     "shell.execute_reply": "2025-04-06T20:02:07.535582Z"
    },
    "papermill": {
     "duration": 0.017669,
     "end_time": "2025-04-06T20:02:07.537439",
     "exception": false,
     "start_time": "2025-04-06T20:02:07.519770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    experiment_name: Optional[str] = \"seq2seq_transformer\"\n",
    "    root_dir: Optional[str] = \"./\"\n",
    "    device: Optional[str] = \"cuda:0\"\n",
    "        \n",
    "    #training parameters\n",
    "    epochs: Optional[int] = 10\n",
    "    seed: Optional[int] = 42\n",
    "    use_half_precision: Optional[bool] = True\n",
    "\n",
    "    # scheduler parameters\n",
    "    scheduler_type: Optional[str] = \"cosine_annealing_warm_restart\" # multi_step or none\n",
    "    T_0: Optional[int] = 10\n",
    "    T_mult: Optional[int] = 1\n",
    "\n",
    "    # optimizer parameters\n",
    "    optimizer_type: Optional[str] = \"adam\" # sgd or adam\n",
    "    optimizer_lr: Optional[float] = 0.0005   \n",
    "    optimizer_momentum: Optional[float] = 0.9\n",
    "    optimizer_weight_decay: Optional[float] = 0.0001\n",
    "    optimizer_no_decay: Optional[list] = field(default_factory=list)\n",
    "    clip_grad_norm: Optional[float] = -1\n",
    "        \n",
    "    # Model Parameters\n",
    "    model_name: Optional[str] = \"seq2seq_transformer\"\n",
    "    hybrid: Optional[bool] = True\n",
    "    embedding_size: Optional[int] = 64\n",
    "    hidden_dim: Optional[int] = 64\n",
    "    pff_dim: Optional[int] = 512\n",
    "    nhead: Optional[int] = 8\n",
    "    num_encoder_layers: Optional[int] = 2\n",
    "    num_decoder_layers: Optional[int] = 6\n",
    "    dropout: Optional[int] = 0.2\n",
    "    pretrain: Optional[bool] = False\n",
    "    input_emb_size: Optional[int] = 64\n",
    "    max_input_points: Optional[int] = 210\n",
    "    src_vocab_size: Optional[int] = 32\n",
    "    tgt_vocab_size: Optional[int] = 22\n",
    "\n",
    "    # Criterion\n",
    "    criterion: Optional[str] = \"cross_entropy\"\n",
    "        \n",
    "    def print_config(self):\n",
    "        print(\"=\"*50+\"\\nConfig\\n\"+\"=\"*50)\n",
    "        for field in fields(self):\n",
    "            print(field.name.ljust(30), getattr(self, field.name))\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def save(self, root_dir):\n",
    "        path = root_dir + \"/config.txt\"\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"=\"*50+\"\\nConfig\\n\"+\"=\"*50 + \"\\n\")\n",
    "            for field in fields(self):\n",
    "                f.write(field.name.ljust(30) + \": \" + str(getattr(self, field.name)) + \"\\n\")\n",
    "            f.write(\"=\"*50)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1561449",
   "metadata": {
    "papermill": {
     "duration": 0.006251,
     "end_time": "2025-04-06T20:02:07.550465",
     "exception": false,
     "start_time": "2025-04-06T20:02:07.544214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787d7038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:07.563970Z",
     "iopub.status.busy": "2025-04-06T20:02:07.563744Z",
     "iopub.status.idle": "2025-04-06T20:02:07.597270Z",
     "shell.execute_reply": "2025-04-06T20:02:07.596380Z"
    },
    "papermill": {
     "duration": 0.041679,
     "end_time": "2025-04-06T20:02:07.598632",
     "exception": false,
     "start_time": "2025-04-06T20:02:07.556953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pth = '/kaggle/input/final-data-fr/final_data_4999.csv'\n",
    "\n",
    "df = pd.read_csv(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a2517ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:07.612402Z",
     "iopub.status.busy": "2025-04-06T20:02:07.612190Z",
     "iopub.status.idle": "2025-04-06T20:02:07.615948Z",
     "shell.execute_reply": "2025-04-06T20:02:07.615335Z"
    },
    "papermill": {
     "duration": 0.011818,
     "end_time": "2025-04-06T20:02:07.617110",
     "exception": false,
     "start_time": "2025-04-06T20:02:07.605292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spt(i, order=4):\n",
    "    expr = sympify(df['expansion'].iloc[i]).evalf(4).as_poly()\n",
    "    coeffs = expr.all_coeffs()[::-1]\n",
    "    if len(coeffs) < order + 1:\n",
    "        coeffs += [0]*(order - len(coeffs) + 1)\n",
    "        \n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c88ef4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:07.630786Z",
     "iopub.status.busy": "2025-04-06T20:02:07.630577Z",
     "iopub.status.idle": "2025-04-06T20:02:20.733207Z",
     "shell.execute_reply": "2025-04-06T20:02:20.732284Z"
    },
    "papermill": {
     "duration": 13.11089,
     "end_time": "2025-04-06T20:02:20.734524",
     "exception": false,
     "start_time": "2025-04-06T20:02:07.623634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4999/4999 [00:13<00:00, 381.81it/s]\n"
     ]
    }
   ],
   "source": [
    "df_clean = pd.DataFrame(columns = df.columns)\n",
    "coeffs = []\n",
    "for idx, row in tqdm(df.iterrows(),total=len(df)):\n",
    "    try:\n",
    "        coeff = spt(idx)\n",
    "        coeffs.append(coeff)\n",
    "        df_clean.loc[len(df_clean)] = row\n",
    "    except Exception as ex:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d2369f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:20.759332Z",
     "iopub.status.busy": "2025-04-06T20:02:20.759064Z",
     "iopub.status.idle": "2025-04-06T20:02:20.763210Z",
     "shell.execute_reply": "2025-04-06T20:02:20.762598Z"
    },
    "papermill": {
     "duration": 0.017441,
     "end_time": "2025-04-06T20:02:20.764296",
     "exception": false,
     "start_time": "2025-04-06T20:02:20.746855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean['coefficients'] = coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb25ea56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:20.788012Z",
     "iopub.status.busy": "2025-04-06T20:02:20.787800Z",
     "iopub.status.idle": "2025-04-06T20:02:21.801853Z",
     "shell.execute_reply": "2025-04-06T20:02:21.801129Z"
    },
    "papermill": {
     "duration": 1.027529,
     "end_time": "2025-04-06T20:02:21.803418",
     "exception": false,
     "start_time": "2025-04-06T20:02:20.775889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "    val_size = val_ratio / (val_ratio + test_ratio)\n",
    "    val_data, test_data = train_test_split(temp_data, train_size=val_size, random_state=42)\n",
    "\n",
    "    data = {\n",
    "        'train': train_data,\n",
    "        'valid': val_data,\n",
    "        'test': test_data\n",
    "    }\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82516447",
   "metadata": {
    "papermill": {
     "duration": 0.011332,
     "end_time": "2025-04-06T20:02:21.826794",
     "exception": false,
     "start_time": "2025-04-06T20:02:21.815462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d528f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:21.850746Z",
     "iopub.status.busy": "2025-04-06T20:02:21.850345Z",
     "iopub.status.idle": "2025-04-06T20:02:21.853875Z",
     "shell.execute_reply": "2025-04-06T20:02:21.853293Z"
    },
    "papermill": {
     "duration": 0.016758,
     "end_time": "2025-04-06T20:02:21.855040",
     "exception": false,
     "start_time": "2025-04-06T20:02:21.838282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_exponent(expr: str) -> str:\n",
    "\n",
    "    expr = re.sub(r'(\\b\\w+\\b)\\s*\\*\\*\\s*([2-4])', r'<\\1^\\2>', expr)\n",
    " \n",
    "    expr = re.sub(r'(?<![\\^_])\\bx\\b(?![\\^_])', '<x^1>', expr)\n",
    "\n",
    "    return expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c34bd5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:21.879254Z",
     "iopub.status.busy": "2025-04-06T20:02:21.878992Z",
     "iopub.status.idle": "2025-04-06T20:02:21.884253Z",
     "shell.execute_reply": "2025-04-06T20:02:21.883541Z"
    },
    "papermill": {
     "duration": 0.018897,
     "end_time": "2025-04-06T20:02:21.885382",
     "exception": false,
     "start_time": "2025-04-06T20:02:21.866485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dec_preproc(input_str, num_token='<NUM>'):\n",
    "    \n",
    "    exp = input_str\n",
    "    e = sympy.Symbol('e')\n",
    "    expr = str(sympify(exp).evalf(6,subs={'e':sympy.core.numbers.E})).replace(' ','')\n",
    "    expr = replace_exponent(expr)\n",
    "    expr_arr = expr.replace('+', ' + ').replace('*',' * ').replace('-',' - ').split(' ')\n",
    "\n",
    "    expr_mod = []\n",
    "\n",
    "    def check_float(f):\n",
    "        try:\n",
    "            _ = float(f)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    for i in expr_arr:\n",
    "        if i == '':\n",
    "            continue\n",
    "\n",
    "        if check_float(i) or check_float(i[:-1]):\n",
    "            for char in str(i):\n",
    "                expr_mod.append(char)\n",
    "        else:\n",
    "            expr_mod.append(i)\n",
    "    return expr_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e293fe",
   "metadata": {
    "papermill": {
     "duration": 0.012258,
     "end_time": "2025-04-06T20:02:21.910779",
     "exception": false,
     "start_time": "2025-04-06T20:02:21.898521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f87a132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:21.935852Z",
     "iopub.status.busy": "2025-04-06T20:02:21.935586Z",
     "iopub.status.idle": "2025-04-06T20:02:21.940026Z",
     "shell.execute_reply": "2025-04-06T20:02:21.939219Z"
    },
    "papermill": {
     "duration": 0.018545,
     "end_time": "2025-04-06T20:02:21.941406",
     "exception": false,
     "start_time": "2025-04-06T20:02:21.922861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_vocab = []\n",
    "\n",
    "src_vocab += ['<PAD>', '<SOS>', '<UNK>', 'x']\n",
    "src_vocab += [str(i) for i in range(10)]\n",
    "src_vocab += ['pi', 's+', 's-', 'E']\n",
    "src_vocab += ['add', 'mul', 'pow']\n",
    "src_vocab += ['sin', 'cos', 'tan', 'cot']\n",
    "src_vocab += ['asin', 'acos', 'atan', 'acot']\n",
    "src_vocab += ['ln', 'exp']\n",
    "src_vocab += ['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21315c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:21.966716Z",
     "iopub.status.busy": "2025-04-06T20:02:21.966479Z",
     "iopub.status.idle": "2025-04-06T20:02:21.970502Z",
     "shell.execute_reply": "2025-04-06T20:02:21.969716Z"
    },
    "papermill": {
     "duration": 0.018004,
     "end_time": "2025-04-06T20:02:21.971718",
     "exception": false,
     "start_time": "2025-04-06T20:02:21.953714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_vocab = []\n",
    "\n",
    "tgt_vocab += ['<PAD>', '<SOS>', '<UNK>']\n",
    "tgt_vocab += [f'<x^{i}>' for i in range(1,5)]\n",
    "tgt_vocab += [str(i) for i in range(10)]\n",
    "tgt_vocab += ['*', '+', '-', '.']\n",
    "tgt_vocab += ['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "135cb1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:21.996064Z",
     "iopub.status.busy": "2025-04-06T20:02:21.995844Z",
     "iopub.status.idle": "2025-04-06T20:02:22.000876Z",
     "shell.execute_reply": "2025-04-06T20:02:22.000147Z"
    },
    "papermill": {
     "duration": 0.018072,
     "end_time": "2025-04-06T20:02:22.002001",
     "exception": false,
     "start_time": "2025-04-06T20:02:21.983929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_vocab), len(src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ca1d14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.025802Z",
     "iopub.status.busy": "2025-04-06T20:02:22.025599Z",
     "iopub.status.idle": "2025-04-06T20:02:22.035834Z",
     "shell.execute_reply": "2025-04-06T20:02:22.035157Z"
    },
    "papermill": {
     "duration": 0.023465,
     "end_time": "2025-04-06T20:02:22.036934",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.013469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        # Initialize with default vocabulary if none provided\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Create mappings\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "        self.idx_to_token = {idx: token for idx, token in enumerate(self.vocab)}\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_idx = self.token_to_idx['<PAD>']\n",
    "        self.sos_idx = self.token_to_idx['<SOS>']\n",
    "        self.eos_idx = self.token_to_idx['<EOS>']\n",
    "        self.unk_idx = self.token_to_idx['<UNK>']\n",
    "    \n",
    "    def encode(self, tokens, add_special_tokens=True, max_length=None):\n",
    "        \"\"\"\n",
    "        Encode a list of tokens into indices\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): List of tokens to encode\n",
    "            add_special_tokens (bool): Whether to add SOS and EOS tokens\n",
    "            max_length (int, optional): Maximum length to pad/truncate to\n",
    "            \n",
    "        Returns:\n",
    "            list: List of token indices\n",
    "        \"\"\"\n",
    "        if add_special_tokens:\n",
    "            tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        indices = [self.token_to_idx.get(token, self.unk_idx) for token in tokens]\n",
    "        \n",
    "        # Handle padding/truncation if max_length specified\n",
    "        if max_length is not None:\n",
    "            if len(indices) < max_length:\n",
    "                # Pad sequence\n",
    "                indices += [self.pad_idx] * (max_length - len(indices))\n",
    "            else:\n",
    "                # Truncate sequence\n",
    "                indices = indices[:max_length]\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices, remove_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Decode a list of indices back into tokens\n",
    "        \n",
    "        Args:\n",
    "            indices (list): List of indices to decode\n",
    "            remove_special_tokens (bool): Whether to remove special tokens\n",
    "            \n",
    "        Returns:\n",
    "            list: List of decoded tokens\n",
    "        \"\"\"\n",
    "        # Convert indices to tokens\n",
    "        tokens = [self.idx_to_token.get(idx, '<UNK>') for idx in indices]\n",
    "        \n",
    "        # Remove special tokens if requested\n",
    "        if remove_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in ['<PAD>', '<SOS>', '<EOS>']]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def batch_encode(self, batch_tokens, add_special_tokens=True, max_length=None, return_tensors=False):\n",
    "        \"\"\"\n",
    "        Encode a batch of token lists\n",
    "        \n",
    "        Args:\n",
    "            batch_tokens (list): List of token lists to encode\n",
    "            add_special_tokens (bool): Whether to add SOS and EOS tokens\n",
    "            max_length (int, optional): Maximum length to pad/truncate to\n",
    "            return_tensors (bool): Whether to return PyTorch tensors\n",
    "            \n",
    "        Returns:\n",
    "            list or torch.Tensor: Batch of encoded sequences\n",
    "        \"\"\"\n",
    "        encoded_batch = [self.encode(tokens, add_special_tokens, max_length) for tokens in batch_tokens]\n",
    "        \n",
    "        # If max_length not specified, pad to the longest sequence in batch\n",
    "        if max_length is None and encoded_batch:\n",
    "            max_len = max(len(seq) for seq in encoded_batch)\n",
    "            encoded_batch = [seq + [self.pad_idx] * (max_len - len(seq)) for seq in encoded_batch]\n",
    "        \n",
    "        # Convert to tensors if requested\n",
    "        if return_tensors:\n",
    "            import torch\n",
    "            encoded_batch = torch.tensor(encoded_batch, dtype=torch.long)\n",
    "        \n",
    "        return encoded_batch\n",
    "    \n",
    "    def save_vocabulary(self, filepath):\n",
    "        \"\"\"Save vocabulary to a file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            for token in self.vocab:\n",
    "                f.write(f\"{token}\\n\")\n",
    "        print(f\"Vocabulary saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, filepath):\n",
    "        \"\"\"Load vocabulary from a file\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            vocab = [line.strip() for line in f]\n",
    "        return cls(vocab)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary\"\"\"\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def add_tokens(self, new_tokens):\n",
    "        \"\"\"\n",
    "        Add new tokens to the vocabulary\n",
    "        \n",
    "        Args:\n",
    "            new_tokens (list): List of tokens to add\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of tokens added\n",
    "        \"\"\"\n",
    "        tokens_added = 0\n",
    "        for token in new_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab.append(token)\n",
    "                self.token_to_idx[token] = len(self.vocab) - 1\n",
    "                self.idx_to_token[len(self.vocab) - 1] = token\n",
    "                tokens_added += 1\n",
    "        \n",
    "        return tokens_added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72627f",
   "metadata": {
    "papermill": {
     "duration": 0.011569,
     "end_time": "2025-04-06T20:02:22.060512",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.048943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "303d210f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.084494Z",
     "iopub.status.busy": "2025-04-06T20:02:22.084268Z",
     "iopub.status.idle": "2025-04-06T20:02:22.089852Z",
     "shell.execute_reply": "2025-04-06T20:02:22.089122Z"
    },
    "papermill": {
     "duration": 0.018848,
     "end_time": "2025-04-06T20:02:22.091037",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.072189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Taylor_data(Dataset):\n",
    "    \n",
    "    def __init__(self, df, src_vocab, tgt_vocab):\n",
    "        self.enc_tokenizer = Tokenizer(src_vocab)\n",
    "        self.dec_tokenizer = Tokenizer(tgt_vocab)\n",
    "\n",
    "        self.df = df\n",
    "        \n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        self.build_dataset()\n",
    "    \n",
    "    def build_dataset(self):\n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            if len(row['prefix']) > 200:\n",
    "                continue\n",
    "                \n",
    "            tgt_preproc = dec_preproc(row['expansion'])\n",
    "            src_ids = self.enc_tokenizer.encode(eval(row['prefix']))\n",
    "            tgt_ids = self.dec_tokenizer.encode(tgt_preproc)\n",
    "            \n",
    "            self.src.append(src_ids)\n",
    "            self.tgt.append(tgt_ids)\n",
    "        \n",
    "        print('Built Dataset')\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return torch.tensor(self.src[idx]).long(), torch.tensor(self.tgt[idx]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bf1812f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.114623Z",
     "iopub.status.busy": "2025-04-06T20:02:22.114409Z",
     "iopub.status.idle": "2025-04-06T20:02:22.117986Z",
     "shell.execute_reply": "2025-04-06T20:02:22.117412Z"
    },
    "papermill": {
     "duration": 0.016517,
     "end_time": "2025-04-06T20:02:22.119055",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.102538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch, num_batch = [], [], []\n",
    "    for (src_sample, tgt_sample) in batch:\n",
    "        \n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "    \n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef960e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.142762Z",
     "iopub.status.busy": "2025-04-06T20:02:22.142550Z",
     "iopub.status.idle": "2025-04-06T20:02:22.146441Z",
     "shell.execute_reply": "2025-04-06T20:02:22.145852Z"
    },
    "papermill": {
     "duration": 0.017021,
     "end_time": "2025-04-06T20:02:22.147632",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.130611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(datasets, train_bs, val_bs, test_bs):\n",
    "    \"\"\"\n",
    "    Get data loaders for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "    - datasets: Dictionary containing train, validation, and test datasets\n",
    "    - train_bs: Batch size for training\n",
    "    - val_bs: Batch size for validation\n",
    "    - test_bs: Batch size for testing\n",
    "\n",
    "    Returns:\n",
    "    - dataloaders: Dictionary containing train, validation, and test data loaders\n",
    "    \"\"\"\n",
    "    train_dataloader = DataLoader(datasets['train'], batch_size=train_bs,\n",
    "                                  shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(datasets['valid'], batch_size=val_bs,\n",
    "                                  shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(datasets['test'], batch_size=test_bs,\n",
    "                                  shuffle=False, num_workers=2, pin_memory=False, collate_fn=collate_fn)\n",
    "    \n",
    "    dataloaders = {\n",
    "        \"train\":train_dataloader,\n",
    "        \"test\":test_dataloader,\n",
    "        \"valid\":val_dataloader\n",
    "        }\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e9d03",
   "metadata": {
    "papermill": {
     "duration": 0.011196,
     "end_time": "2025-04-06T20:02:22.170379",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.159183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaa4d251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.194223Z",
     "iopub.status.busy": "2025-04-06T20:02:22.193972Z",
     "iopub.status.idle": "2025-04-06T20:02:22.202229Z",
     "shell.execute_reply": "2025-04-06T20:02:22.201439Z"
    },
    "papermill": {
     "duration": 0.021734,
     "end_time": "2025-04-06T20:02:22.203433",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.181699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt, device):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (torch.zeros((src.shape[0], src_seq_len), device=device)).type(torch.bool)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX)\n",
    "    tgt_mask = tgt_mask\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def sequence_accuracy(y_pred, y_true):\n",
    "\n",
    "    count = 0\n",
    "    total = len(y_pred)\n",
    "    for (predicted_tokens, original_tokens) in zip(y_pred, y_true):\n",
    "        original_tokens = original_tokens.tolist()\n",
    "        predicted_tokens = predicted_tokens.tolist()\n",
    "        if original_tokens == predicted_tokens:\n",
    "            count = count+1\n",
    "\n",
    "    return count/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ecb53",
   "metadata": {
    "papermill": {
     "duration": 0.011335,
     "end_time": "2025-04-06T20:02:22.226175",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.214840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d70a1719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.250002Z",
     "iopub.status.busy": "2025-04-06T20:02:22.249776Z",
     "iopub.status.idle": "2025-04-06T20:02:22.264385Z",
     "shell.execute_reply": "2025-04-06T20:02:22.263783Z"
    },
    "papermill": {
     "duration": 0.0279,
     "end_time": "2025-04-06T20:02:22.265565",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.237665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/neerajanand321/SYMBA_Pytorch/blob/main/models/seq2seq_transformer.py\n",
    "class TokenEmbedding(nn.Module):\n",
    "    ''' helper Module to convert tensor of input indices into corresponding tensor of token embeddings'''\n",
    "    \n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    ''' helper Module that adds positional encoding to the token embedding to introduce a notion of word order.'''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''Seq2Seq Network'''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 input_emb_size: int,\n",
    "                 max_input_points: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1,):\n",
    "        super(Model, self).__init__()\n",
    "        # self.transformer = Transformer(d_model=emb_size,\n",
    "        #                                nhead=nhead,\n",
    "        #                                num_encoder_layers=num_encoder_layers,\n",
    "        #                                num_decoder_layers=num_decoder_layers,\n",
    "        #                                dim_feedforward=dim_feedforward,\n",
    "        #                                dropout=dropout,\n",
    "        #                                batch_first=True)\n",
    "        self.encoder = nn.LSTM(input_size = emb_size,\n",
    "                                hidden_size = dim_feedforward,\n",
    "                                num_layers = num_encoder_layers, \n",
    "                                batch_first = True, \n",
    "                                dropout = dropout,\n",
    "                                proj_size = emb_size\n",
    "                        )\n",
    "        self.decoder = nn.LSTM(input_size = emb_size,\n",
    "                                hidden_size = dim_feedforward,\n",
    "                                num_layers = num_encoder_layers, \n",
    "                                batch_first = True, \n",
    "                                dropout = dropout,\n",
    "                               proj_size = emb_size\n",
    "                              )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor,\n",
    "               teacher_forcing_ratio = 0.9):\n",
    "        \n",
    "        src_emb = self.src_tok_emb(src)\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        target_length = trg.shape[1]\n",
    "        target_vocab_size = self.generator.out_features\n",
    "        \n",
    "        # Initialize tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, target_length, target_vocab_size).to(trg.device)\n",
    "        # Encode the source sequence\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        _, (hidden, cell) = self.encoder(src_emb)\n",
    "        \n",
    "        # First decoder input is the first token of target sequence (SOS token)\n",
    "        decoder_input = trg[:, 0:1]\n",
    "        \n",
    "        # Teacher forcing implementation\n",
    "        for t in range(1, target_length):\n",
    "            # Get embedding for current input\n",
    "            decoder_input_emb = self.positional_encoding(self.tgt_tok_emb(decoder_input))\n",
    "\n",
    "            # Pass through decoder - keeping batch dimension intact\n",
    "            decoder_output, (hidden, cell) = self.decoder(decoder_input_emb, (hidden, cell))\n",
    "            # Generate prediction\n",
    "            prediction = self.generator(decoder_output)\n",
    "            \n",
    "            # Store prediction for this timestep\n",
    "            outputs[:, t-1, :] = prediction.squeeze(1)\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            \n",
    "            if use_teacher_forcing:\n",
    "                # Use actual next token from target\n",
    "                decoder_input = trg[:, t:t+1]\n",
    "            else:\n",
    "                # Use highest probability predicted token\n",
    "                topv, topi = prediction.squeeze(1).topk(1)\n",
    "                decoder_input = topi.detach()  # detach from history as input\n",
    "                \n",
    "        # Handle last prediction\n",
    "        decoder_input_emb = self.positional_encoding(self.tgt_tok_emb(decoder_input))\n",
    "        decoder_output, _ = self.decoder(decoder_input_emb, (hidden, cell))\n",
    "        prediction = self.generator(decoder_output)\n",
    "        outputs[:, target_length-1, :] = prediction.squeeze(1)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        # outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "        #                         src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        # return self.generator(outs)\n",
    "    def encode(self, src: Tensor, src_mask: Tensor = None):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        return self.encoder(src_emb)\n",
    "    \n",
    "    def decode(self, tgt: Tensor, memory: Tensor = None, tgt_mask: Tensor = None):\n",
    "        \"\"\"\n",
    "        Decode function for inference\n",
    "        memory should be a tuple of (hidden_state, cell_state) from encoder\n",
    "        \"\"\"\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "            \n",
    "        if isinstance(memory, tuple) and len(memory) == 2:\n",
    "            # If memory is provided as (hidden, cell) tuple\n",
    "            hidden_state, cell_state = memory\n",
    "            return self.decoder(tgt_emb, (hidden_state, cell_state))\n",
    "        else:\n",
    "            # Fallback if memory isn't properly formatted\n",
    "            return self.decoder(tgt_emb)\n",
    "\n",
    "    # def encode(self, src: Tensor, src_mask: Tensor):\n",
    "    #     return self.encoder(self.src_tok_emb(src), src_mask)\n",
    "\n",
    "    # def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "    #     return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378b751",
   "metadata": {
    "papermill": {
     "duration": 0.011199,
     "end_time": "2025-04-06T20:02:22.288489",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.277290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da57f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.312092Z",
     "iopub.status.busy": "2025-04-06T20:02:22.311831Z",
     "iopub.status.idle": "2025-04-06T20:02:22.323670Z",
     "shell.execute_reply": "2025-04-06T20:02:22.322921Z"
    },
    "papermill": {
     "duration": 0.024934,
     "end_time": "2025-04-06T20:02:22.324864",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.299930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"\n",
    "    Predictor class for generating predictions using a trained model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize Predictor object.\n",
    "\n",
    "        Args:\n",
    "        - config: Configuration object containing model parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "\n",
    "        # Get the model\n",
    "        self.model = self.get_model()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Load the best checkpoint\n",
    "        self.logs_dir = os.path.join(self.config.root_dir, self.config.experiment_name)\n",
    "        path = os.path.join(self.logs_dir, \"best_checkpoint.pth\")\n",
    "        self.model.load_state_dict(torch.load(path)[\"state_dict\"])\n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_model(self):\n",
    "        if self.config.model_name == \"seq2seq_transformer\":\n",
    "            model = Model(num_encoder_layers=self.config.num_encoder_layers,\n",
    "                          num_decoder_layers=self.config.num_decoder_layers,\n",
    "                          emb_size=self.config.embedding_size,\n",
    "                          nhead=self.config.nhead,\n",
    "                          src_vocab_size=self.config.src_vocab_size,\n",
    "                          tgt_vocab_size=self.config.tgt_vocab_size,\n",
    "                          input_emb_size=self.config.input_emb_size,\n",
    "                          max_input_points=self.config.max_input_points,\n",
    "                          )\n",
    "\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def greedy_decode(self, src, src_mask, max_len, start_symbol, src_padding_mask=None):\n",
    "        src = src.to(self.device)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src_padding_mask = src_padding_mask.to(self.device)\n",
    "        dim = 1\n",
    "\n",
    "        memory = self.model.encode(src, src_mask)\n",
    "        memory = memory.to(self.device)\n",
    "        dim = 1\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(self.device)\n",
    "        for i in range(max_len-1):\n",
    "\n",
    "            tgt_mask = (self.generate_square_subsequent_mask(ys.size(1), self.device).type(torch.bool)).to(self.device)\n",
    "\n",
    "            out = self.model.decode(ys, memory, tgt_mask)\n",
    "            prob = self.model.generator(out[:, -1])\n",
    "\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=dim)\n",
    "            if next_word == EOS_IDX:\n",
    "                break\n",
    "\n",
    "        return ys\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        \n",
    "        if self.config.model_name == \"seq2seq_transformer\":\n",
    "            src = x\n",
    "            num_tokens = src.shape[1]\n",
    "\n",
    "            src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "            src_padding_mask = torch.zeros(1, num_tokens).type(torch.bool)\n",
    "            tgt_tokens = self.greedy_decode(src, src_mask, max_len=256, start_symbol=BOS_IDX, src_padding_mask=src_padding_mask).flatten()\n",
    "\n",
    "            return tgt_tokens\n",
    "        else:\n",
    "            ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(self.device)\n",
    "            e_mask = torch.zeros(1, x.shape[1]).type(torch.bool).to(self.device)\n",
    "            memory = self.model.encoder(x, e_mask)\n",
    "\n",
    "            for idx in range(1, 256):\n",
    "                d_mask = torch.triu(torch.full((ys.size(1), ys.size(1)), float('-inf')), diagonal=1).to(self.device)\n",
    "                d_out = self.model.decoder(ys, memory, e_mask, d_mask)\n",
    "\n",
    "                prob = self.model.generator(d_out[:, -1])\n",
    "                _, next_word = torch.max(prob, dim=1)\n",
    "                next_word = next_word.item()\n",
    "                ys = torch.cat([ys, torch.ones(1, 1).type_as(x.data).fill_(next_word)], dim=1)\n",
    "                if next_word == EOS_IDX:\n",
    "                    break\n",
    "\n",
    "            return ys.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3539eb7",
   "metadata": {
    "papermill": {
     "duration": 0.011267,
     "end_time": "2025-04-06T20:02:22.347601",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.336334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaca1039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.381412Z",
     "iopub.status.busy": "2025-04-06T20:02:22.381165Z",
     "iopub.status.idle": "2025-04-06T20:02:22.405775Z",
     "shell.execute_reply": "2025-04-06T20:02:22.405108Z"
    },
    "papermill": {
     "duration": 0.046606,
     "end_time": "2025-04-06T20:02:22.406996",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.360390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for training and evaluating a PyTorch model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, dataloaders):\n",
    "        \"\"\"\n",
    "        Initialize Trainer object.\n",
    "\n",
    "        Args:\n",
    "        - config: Configuration object containing training parameters\n",
    "        - dataloaders: Dictionary containing data loaders for train, validation, and test sets\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "        seed_everything(self.config.seed)\n",
    "\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        if self.config.use_half_precision:\n",
    "            self.dtype = torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        # Initialize model, optimizer, scheduler, and criterion\n",
    "        self.model = self.get_model()\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = self.get_optimizer()\n",
    "        self.scheduler = self.get_scheduler()\n",
    "        self.criterion = self.get_criterion()\n",
    "\n",
    "        # Initialize training-related variables\n",
    "        self.current_epoch = 0\n",
    "        self.best_accuracy = -1\n",
    "        self.best_val_loss = 1e6\n",
    "        self.train_loss_list = []\n",
    "        self.valid_loss_list = []\n",
    "        self.valid_accuracy_tok_list = []\n",
    "\n",
    "        # Create directory for saving logs\n",
    "        self.logs_dir = os.path.join(self.config.root_dir, self.config.experiment_name)\n",
    "        os.makedirs(self.logs_dir, exist_ok=True)\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the model based on the configuration.\n",
    "        \"\"\"\n",
    "        if self.config.model_name == \"seq2seq_transformer\":\n",
    "            model = Model(num_encoder_layers=self.config.num_encoder_layers,\n",
    "                          num_decoder_layers=self.config.num_decoder_layers,\n",
    "                          emb_size=self.config.embedding_size,\n",
    "                          nhead=self.config.nhead,\n",
    "                          src_vocab_size=self.config.src_vocab_size,\n",
    "                          tgt_vocab_size=self.config.tgt_vocab_size,\n",
    "                          input_emb_size=self.config.input_emb_size,\n",
    "                          max_input_points=self.config.max_input_points,\n",
    "                          )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the optimizer based on the configuration.\n",
    "        \"\"\"\n",
    "        optimizer_parameters = self.model.parameters()\n",
    "\n",
    "        if self.config.optimizer_type == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(optimizer_parameters, lr=self.config.optimizer_lr, momentum=self.config.optimizer_momentum,)\n",
    "        elif self.config.optimizer_type == \"adam\":\n",
    "            optimizer = torch.optim.Adam(optimizer_parameters, lr=self.config.optimizer_lr, eps=1e-8, weight_decay=self.config.optimizer_weight_decay)\n",
    "        elif self.config.optimizer_type == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(optimizer_parameters, lr=self.config.optimizer_lr, eps=1e-8, weight_decay=self.config.optimizer_weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def get_scheduler(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the learning rate scheduler based on the configuration.\n",
    "        \"\"\"\n",
    "        if self.config.scheduler_type == \"multi_step\":\n",
    "            scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=self.config.scheduler_milestones, gamma=self.config.scheduler_gamma)\n",
    "        elif self.config.scheduler_type == \"reduce_lr_on_plateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=2)\n",
    "        elif self.config.scheduler_type == \"cosine_annealing_warm_restart\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, self.config.T_0, self.config.T_mult)\n",
    "        elif self.config.scheduler_type == \"none\":\n",
    "            scheduler = None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return scheduler\n",
    "\n",
    "    \n",
    "    def get_criterion(self):\n",
    "        \"\"\"\n",
    "        Initialize and return the loss function based on the configuration.\n",
    "        \"\"\"\n",
    "        if self.config.criterion == \"cross_entropy\":\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return criterion\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        pbar = tqdm(self.dataloaders['train'], total=len(self.dataloaders['train']))\n",
    "        pbar.set_description(f\"[{self.current_epoch+1}/{self.config.epochs}] Train\")\n",
    "        running_loss = AverageMeter()\n",
    "        for src, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "\n",
    "            bs = src.size(0)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "                if self.config.model_name == \"seq2seq_transformer\":\n",
    "                    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:, :-1], self.device)\n",
    "                    logits = self.model(src, tgt[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "                    loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "                else:\n",
    "                    logits = self.model(src, tgt[:, :-1])\n",
    "                    loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "                \n",
    "            running_loss.update(loss.item(), bs)\n",
    "            pbar.set_postfix(loss=running_loss.avg)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "\n",
    "            if self.config.clip_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_grad_norm)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "        return running_loss.avg\n",
    "\n",
    "    def evaluate(self, phase):\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation or test data.\n",
    "\n",
    "        Args:\n",
    "        - phase: Phase of evaluation, either \"valid\" or \"test\".\n",
    "\n",
    "        Returns:\n",
    "        - Tuple containing average token accuracy and average loss.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        pbar = tqdm(self.dataloaders[phase], total=len(self.dataloaders[phase]))\n",
    "        pbar.set_description(f\"[{self.current_epoch+1}/{self.config.epochs}] {phase.capitalize()}\")\n",
    "        running_loss = AverageMeter()\n",
    "        running_acc_tok = AverageMeter()\n",
    "        \n",
    "        \n",
    "        for src, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "            bs = src.size(0)\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "                if self.config.model_name == \"seq2seq_transformer\":\n",
    "                    with torch.no_grad():\n",
    "                        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:, :-1], self.device)\n",
    "                        logits = self.model(src, tgt[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "                        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(src, tgt[:, :-1])\n",
    "                        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "            y_pred = torch.argmax(logits.reshape(-1, logits.shape[-1]), 1)\n",
    "            correct = (y_pred == tgt[:, 1:].reshape(-1)).cpu().numpy().mean()\n",
    "            \n",
    "            running_loss.update(loss.item(), bs)\n",
    "            running_acc_tok.update(correct, bs)\n",
    "            \n",
    "        return running_acc_tok.avg, running_loss.avg\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \"\"\"\n",
    "        start_epoch = self.current_epoch\n",
    "        for self.current_epoch in range(start_epoch, self.config.epochs):\n",
    "            training_loss = self.train_one_epoch() \n",
    "            valid_accuracy_tok, valid_loss = self.evaluate(\"valid\")\n",
    "            \n",
    "            self.train_loss_list.append(round(training_loss, 7))\n",
    "            self.valid_loss_list.append(round(valid_loss, 7))\n",
    "            self.valid_accuracy_tok_list.append(round(valid_accuracy_tok, 7))\n",
    "            \n",
    "            if self.scheduler == \"multi_step\":\n",
    "                self.scheduler.step()\n",
    "            elif self.scheduler == \"reduce_lr_on_plateau\":\n",
    "                self.scheduler.step(valid_loss)\n",
    "                \n",
    "            if valid_loss<self.best_val_loss:\n",
    "                self.best_val_loss = valid_loss\n",
    "\n",
    "            self.save_model(\"last_checkpoint.pth\")\n",
    "\n",
    "            if valid_accuracy_tok > self.best_accuracy:\n",
    "                print(f\"==> Best Accuracy improved to {round(valid_accuracy_tok, 7)} from {self.best_accuracy}\")\n",
    "                self.best_accuracy = round(valid_accuracy_tok, 7)\n",
    "                self.save_model(\"best_checkpoint.pth\")\n",
    "            \n",
    "            self.log_results()\n",
    "\n",
    "        \n",
    "    def save_model(self, file_name):\n",
    "        \"\"\"\n",
    "        Save model checkpoints.\n",
    "        \"\"\"\n",
    "        state_dict = self.model.state_dict()\n",
    "        torch.save({\n",
    "                \"epoch\": self.current_epoch + 1,\n",
    "                \"state_dict\": state_dict,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                \"train_loss_list\": self.train_loss_list,\n",
    "                \"valid_loss_list\": self.valid_loss_list,\n",
    "                \"valid_accuracy_tok_list\": self.valid_accuracy_tok_list,\n",
    "            }, os.path.join(self.logs_dir, file_name))\n",
    "\n",
    "    def log_results(self):\n",
    "        \"\"\"\n",
    "        Log training results to a CSV file.\n",
    "        \"\"\"\n",
    "        data_list = [self.train_loss_list, self.valid_loss_list, self.valid_accuracy_tok_list]\n",
    "        column_list = ['train_losses', 'valid_losses', 'token_valid_accuracy']\n",
    "        \n",
    "        df_data = np.array(data_list).T\n",
    "        df = pd.DataFrame(df_data, columns=column_list)\n",
    "        df.to_csv(os.path.join(self.logs_dir, \"logs.csv\"))\n",
    "        \n",
    "    def test_seq_acc(self):\n",
    "        \"\"\"\n",
    "        Evaluate model's sequence accuracy on test data.\n",
    "        \"\"\"\n",
    "        file = os.path.join(self.logs_dir, \"best_checkpoint.pth\")\n",
    "        state_dict = torch.load(file, map_location=self.device)['state_dict']\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        \n",
    "        test_accuracy_tok, _ = self.evaluate(\"test\")\n",
    "        \n",
    "        predictor = Predictor(self.config)\n",
    "        \n",
    "        print(\"Calculating Sequence Accuracy for predictions (1 example per batch)\")\n",
    "        pbar = tqdm(self.dataloaders[\"test\"], total=len(self.dataloaders[\"test\"]))\n",
    "        pbar.set_description(f\"Test\")\n",
    "        \n",
    "        y_preds = []\n",
    "        y_true = []\n",
    "        for src, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.numpy()\n",
    "            bs = src.size(0)\n",
    "            y_pred = predictor.predict(src[0].unsqueeze(0)) #only one example from each batch\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            y_true.append(np.trim_zeros(tgt[0]))\n",
    "        print(y_preds[1], y_true[1])\n",
    "        test_accuracy_seq = sequence_accuracy(y_true, y_preds)\n",
    "        f= open(os.path.join(self.logs_dir, \"score.txt\"),\"w+\")\n",
    "        f.write(f\"Token Accuracy = {(round(test_accuracy_tok, 7))}\\n\")\n",
    "        f.write(f\"Sequence Accuracy = {(round(test_accuracy_seq, 7))}\\n\")\n",
    "        f.close()\n",
    "        print(f\"Test Accuracy: {round(test_accuracy_tok, 7)} | Valid Accuracy: {self.best_accuracy}\") \n",
    "        print(f\"Test Sequence Accuracy: {test_accuracy_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edb63c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:22.477053Z",
     "iopub.status.busy": "2025-04-06T20:02:22.476816Z",
     "iopub.status.idle": "2025-04-06T20:02:31.338620Z",
     "shell.execute_reply": "2025-04-06T20:02:31.337513Z"
    },
    "papermill": {
     "duration": 8.875221,
     "end_time": "2025-04-06T20:02:31.340005",
     "exception": false,
     "start_time": "2025-04-06T20:02:22.464784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3296/3296 [00:05<00:00, 550.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 706/706 [00:01<00:00, 491.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 707/707 [00:01<00:00, 498.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "df_train, df_valid, df_test = split_data(df_clean)\n",
    "datasets = {\n",
    "    'train': Taylor_data(df_train, src_vocab, tgt_vocab),\n",
    "    'valid': Taylor_data(df_valid, src_vocab, tgt_vocab),\n",
    "    'test': Taylor_data(df_test, src_vocab, tgt_vocab)\n",
    "}\n",
    "# dataloaders = get_dataloaders(datasets, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5762b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:31.372611Z",
     "iopub.status.busy": "2025-04-06T20:02:31.372364Z",
     "iopub.status.idle": "2025-04-06T20:02:31.376421Z",
     "shell.execute_reply": "2025-04-06T20:02:31.375301Z"
    },
    "papermill": {
     "duration": 0.021467,
     "end_time": "2025-04-06T20:02:31.377729",
     "exception": false,
     "start_time": "2025-04-06T20:02:31.356262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders = get_dataloaders(datasets, 128, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b742f0db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:31.445542Z",
     "iopub.status.busy": "2025-04-06T20:02:31.445325Z",
     "iopub.status.idle": "2025-04-06T20:02:33.476460Z",
     "shell.execute_reply": "2025-04-06T20:02:33.475781Z"
    },
    "papermill": {
     "duration": 2.048426,
     "end_time": "2025-04-06T20:02:33.477971",
     "exception": false,
     "start_time": "2025-04-06T20:02:31.429545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7948be0243d7>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "trainer = Trainer(config, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b54344a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T20:02:33.511789Z",
     "iopub.status.busy": "2025-04-06T20:02:33.511517Z",
     "iopub.status.idle": "2025-04-06T20:03:22.421374Z",
     "shell.execute_reply": "2025-04-06T20:03:22.420123Z"
    },
    "papermill": {
     "duration": 48.928316,
     "end_time": "2025-04-06T20:03:22.422924",
     "exception": false,
     "start_time": "2025-04-06T20:02:33.494608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/10] Train: 100%|██████████| 25/25 [00:04<00:00,  5.26it/s, loss=1.47]\n",
      "[1/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7647148 from -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/10] Train: 100%|██████████| 25/25 [00:04<00:00,  6.19it/s, loss=0.726]\n",
      "[2/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7978273 from 0.7647148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/10] Train: 100%|██████████| 25/25 [00:03<00:00,  6.36it/s, loss=0.676]\n",
      "[3/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.7979488 from 0.7978273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/10] Train: 100%|██████████| 25/25 [00:04<00:00,  6.24it/s, loss=0.606]\n",
      "[4/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.8404455 from 0.7979488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/10] Train: 100%|██████████| 25/25 [00:03<00:00,  6.36it/s, loss=0.499]\n",
      "[5/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.8550215 from 0.8404455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/10] Train: 100%|██████████| 25/25 [00:04<00:00,  6.07it/s, loss=0.412]\n",
      "[6/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.8684923 from 0.8550215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/10] Train: 100%|██████████| 25/25 [00:03<00:00,  6.31it/s, loss=0.382]\n",
      "[7/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.8723882 from 0.8684923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/10] Train: 100%|██████████| 25/25 [00:03<00:00,  6.29it/s, loss=0.357]\n",
      "[8/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.8902681 from 0.8723882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/10] Train: 100%|██████████| 25/25 [00:03<00:00,  6.41it/s, loss=0.341]\n",
      "[9/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best Accuracy improved to 0.893705 from 0.8902681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/10] Train: 100%|██████████| 25/25 [00:03<00:00,  6.28it/s, loss=0.314]\n",
      "[10/10] Valid: 100%|██████████| 11/11 [00:00<00:00, 14.73it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec675733",
   "metadata": {
    "papermill": {
     "duration": 0.041984,
     "end_time": "2025-04-06T20:03:22.509281",
     "exception": false,
     "start_time": "2025-04-06T20:03:22.467297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7057290,
     "sourceId": 11287173,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 90.445652,
   "end_time": "2025-04-06T20:03:24.273850",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-06T20:01:53.828198",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
